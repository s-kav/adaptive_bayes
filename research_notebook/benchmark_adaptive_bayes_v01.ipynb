{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3abXouJZxXNb",
    "outputId": "4618b8cf-8e5e-4851-b9da-9323853d9f12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting memory_profiler\n",
      "  Downloading memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from memory_profiler) (5.9.5)\n",
      "Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: memory_profiler\n",
      "Successfully installed memory_profiler-0.61.0\n"
     ]
    }
   ],
   "source": [
    "!pip install memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6anN_kdw2rS"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ymWQbsSvww8t"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gzip\n",
    "import shutil\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas.errors import ParserError\n",
    "\n",
    "# Optional GPU memory\n",
    "try:\n",
    "    from cupy.cuda import runtime as cuda_rt\n",
    "    GPU_OK = True\n",
    "except Exception:\n",
    "    GPU_OK = False\n",
    "\n",
    "import psutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "# Baselines\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_OK = True\n",
    "except Exception:\n",
    "    XGB_OK = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_OK = True\n",
    "except Exception:\n",
    "    LGB_OK = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    CAT_OK = True\n",
    "except Exception:\n",
    "    CAT_OK = False\n",
    "\n",
    "from adaptive_bayes import AdaptiveBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9u3m4qcUxA6o"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "PNdWaFuKwy0D"
   },
   "outputs": [],
   "source": [
    "def _gpu_mem_info():\n",
    "    if not GPU_OK:\n",
    "        return None, None\n",
    "    free_b, total_b = cuda_rt.memGetInfo()\n",
    "    return free_b, total_b\n",
    "\n",
    "def _proc_rss_mb():\n",
    "    return psutil.Process(os.getpid()).memory_info().rss / (1024*1024)\n",
    "\n",
    "def _measure_run(fn, *args, **kwargs):\n",
    "    # CPU mem before\n",
    "    rss_before = _proc_rss_mb()\n",
    "    # GPU mem before\n",
    "    free0, total0 = _gpu_mem_info()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    def _wrapped():\n",
    "        return fn(*args, **kwargs)\n",
    "\n",
    "    mem_trace = memory_usage((_wrapped, (), {}), max_iterations=1, interval=0.1, retval=True)\n",
    "    if isinstance(mem_trace, tuple) and len(mem_trace) == 2:\n",
    "        mem_series, ret = mem_trace\n",
    "    else:\n",
    "        mem_series, ret = mem_trace, None\n",
    "\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    rss_after = _proc_rss_mb()\n",
    "    free1, total1 = _gpu_mem_info()\n",
    "    peak_cpu = max(mem_series) - rss_before if mem_series else 0.0\n",
    "    gpu_delta = None\n",
    "    if free0 is not None and free1 is not None:\n",
    "        gpu_delta = (free0 - free1) / (1024*1024)\n",
    "\n",
    "    return {\n",
    "        \"elapsed_s\": elapsed,\n",
    "        \"cpu_rss_mb_before\": rss_before,\n",
    "        \"cpu_peak_mb\": peak_cpu,\n",
    "        \"gpu_mem_delta_mb\": gpu_delta,\n",
    "        \"ret\": ret\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset loaders (paths expected)\n",
    "# ---------------------------\n",
    "def load_creditcard_fraud(path_csv):\n",
    "    # Kaggle: V1..V28 + Time, Amount, Class; binary Class\n",
    "    df = pd.read_csv(path_csv)\n",
    "    y = df['Class'].astype(np.int32).values\n",
    "    X = df.drop(columns=['Class']).values.astype(np.float64)\n",
    "    return X, y\n",
    "\n",
    "def load_higgs(path_gz):\n",
    "    # UCI: CSV.gz with label first, then 28 features\n",
    "    with gzip.open(path_gz, 'rt') as f:\n",
    "        df = pd.read_csv(f, header=None)\n",
    "    y = df.iloc[:, 0].astype(np.int32).values\n",
    "    X = df.iloc[:, 1:].values.astype(np.float64)\n",
    "    return X, y\n",
    "\n",
    "def load_susy(path_gz):\n",
    "    with gzip.open(path_gz, 'rt') as f:\n",
    "        df = pd.read_csv(f, header=None)\n",
    "    y = df.iloc[:, 0].astype(np.int32).values\n",
    "    X = df.iloc[:, 1:].values.astype(np.float64)\n",
    "    return X, y\n",
    "\n",
    "def load_kddcup99(path_csv, drop_cats=True):\n",
    "    # Mixed dtypes; simplify to numeric by one-hot or drop_cats\n",
    "    df = pd.read_csv(path_csv, header=None)\n",
    "    if drop_cats:\n",
    "        # Keep numeric columns only\n",
    "        num_df = df.select_dtypes(include=[np.number])\n",
    "        # Target can be last column or named; assume last is label string -> map to binary (normal vs attack)\n",
    "        # If last column non-numeric, we map\n",
    "        if not np.issubdtype(df.iloc[:, -1].dtype, np.number):\n",
    "            y = (df.iloc[:, -1].astype(str) != 'normal.').astype(np.int32).values\n",
    "        else:\n",
    "            y = df.iloc[:, -1].astype(np.int32).values\n",
    "        X = num_df.iloc[:, :-1].values.astype(np.float64)\n",
    "    else:\n",
    "        # One-hot encode categoricals\n",
    "        y = (df.iloc[:, -1].astype(str) != 'normal.').astype(np.int32).values\n",
    "        X = pd.get_dummies(df.iloc[:, :-1]).values.astype(np.float64)\n",
    "    return X, y\n",
    "\n",
    "def load_covertype(path_csv):\n",
    "    df = pd.read_csv(path_csv)\n",
    "    target_col = 'Cover_Type' if 'Cover_Type' in df.columns else df.columns[-1]\n",
    "    y = np.asarray(df[target_col], dtype=np.int32) - 1\n",
    "    # Convert to binary: class1 vs others to align with AUC, or keep multiclass for accuracy\n",
    "    # Here we keep multiclass; AUC will be skipped for multiclass\n",
    "    X = df.drop(columns=[target_col]).values.astype(np.float64)\n",
    "    return X, y\n",
    "\n",
    "def create_synthetic_hepmass():\n",
    "    \"\"\"Создает синтетический датасет в стиле HEPMASS\"\"\"\n",
    "    print(\"Creating synthetic HEPMASS-like dataset...\")\n",
    "    np.random.seed(42)\n",
    "    n_samples = 500000\n",
    "    n_features = 28\n",
    "\n",
    "    # Создать корреляционные признаки как в физических данных\n",
    "    X = np.random.randn(n_samples, n_features).astype(np.float64)\n",
    "\n",
    "    # Добавить нелинейные взаимодействия для реализма\n",
    "    X[:, 1] = X[:, 0] ** 2 + 0.5 * np.random.randn(n_samples)\n",
    "    X[:, 2] = X[:, 0] * X[:, 1] + 0.3 * np.random.randn(n_samples)\n",
    "\n",
    "    # Создать сложную целевую переменную\n",
    "    signal = (0.3 * X[:, 0] + 0.2 * X[:, 1] - 0.1 * X[:, 2] +\n",
    "              0.15 * X[:, 3] * X[:, 4] + 0.1 * np.sin(X[:, 5]))\n",
    "    noise = 0.5 * np.random.randn(n_samples)\n",
    "    y = (signal + noise > 0).astype(np.int32)\n",
    "\n",
    "    print(f\"Synthetic dataset: X={X.shape}, y={y.shape}, class balance={np.mean(y):.3f}\")\n",
    "    return X, y\n",
    "\n",
    "def load_hepmass(path_csv):\n",
    "    \"\"\"Робастная загрузка HEPMASS с обработкой ошибок парсинга\"\"\"\n",
    "    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
    "    df = None\n",
    "\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            # Попробовать разные варианты парсинга\n",
    "            parsing_options = [\n",
    "                # Стандартный CSV\n",
    "                {'encoding': encoding, 'sep': ','},\n",
    "                # Разделитель - пробел/табуляция\n",
    "                {'encoding': encoding, 'sep': r'\\s+', 'engine': 'python'},\n",
    "                # Пропуск плохих строк\n",
    "                {'encoding': encoding, 'sep': ',', 'on_bad_lines': 'skip'},\n",
    "                # Без заголовка\n",
    "                {'encoding': encoding, 'sep': ',', 'header': None, 'on_bad_lines': 'skip'},\n",
    "            ]\n",
    "\n",
    "            for options in parsing_options:\n",
    "                try:\n",
    "                    df = pd.read_csv(path_csv, **options)\n",
    "                    print(f\"Successfully loaded HEPMASS with encoding: {encoding}, options: {options}\")\n",
    "                    break\n",
    "                except (ParserError, pd.errors.ParserError):\n",
    "                    continue\n",
    "\n",
    "            if df is not None:\n",
    "                break\n",
    "\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        print(\"Failed to load HEPMASS, creating synthetic dataset...\")\n",
    "        return create_synthetic_hepmass()\n",
    "\n",
    "    # Обработка разных форматов колонок\n",
    "    print(f\"HEPMASS loaded: {df.shape}, columns: {list(df.columns)}\")\n",
    "\n",
    "    # Попробовать найти целевую переменную\n",
    "    if '# label' in df.columns:\n",
    "        y = df['# label'].astype(np.int32)\n",
    "        X = df.drop(columns=['# label']).select_dtypes(include=[np.number]).values.astype(np.float64)\n",
    "    elif 'type' in df.columns:\n",
    "        y = df['type'].astype(np.int32)\n",
    "        X = df.drop(columns=['type']).select_dtypes(include=[np.number]).values.astype(np.float64)\n",
    "    elif 'label' in df.columns:\n",
    "        y = df['label'].astype(np.int32)\n",
    "        X = df.drop(columns=['label']).select_dtypes(include=[np.number]).values.astype(np.float64)\n",
    "    else:\n",
    "        # Предполагаем первую или последнюю колонку как target\n",
    "        if df.shape[1] > 1:\n",
    "            # Попробовать последнюю колонку как target\n",
    "            last_col = df.iloc[:, -1]\n",
    "            if last_col.dtype in ['int64', 'float64'] and last_col.nunique() <= 10:\n",
    "                y = last_col.astype(np.int32)\n",
    "                X = df.iloc[:, :-1].select_dtypes(include=[np.number]).values.astype(np.float64)\n",
    "            else:\n",
    "                # Первая колонка как target\n",
    "                y = df.iloc[:, 0].astype(np.int32)\n",
    "                X = df.iloc[:, 1:].select_dtypes(include=[np.number]).values.astype(np.float64)\n",
    "        else:\n",
    "            print(\"Cannot determine target variable, creating synthetic...\")\n",
    "            return create_synthetic_hepmass()\n",
    "\n",
    "    # Проверки корректности\n",
    "    if X.shape[0] == 0 or X.shape[1] == 0:\n",
    "        print(\"Empty feature matrix, creating synthetic...\")\n",
    "        return create_synthetic_hepmass()\n",
    "\n",
    "    # Конвертация меток в бинарные если нужно\n",
    "    if len(np.unique(y)) > 2:\n",
    "        print(f\"Converting {len(np.unique(y))} classes to binary\")\n",
    "        y = (y > np.median(y)).astype(np.int32)\n",
    "\n",
    "    # Убрать NaN/inf значения\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def load_avazu(path_csv, sample_n=None):\n",
    "    # High-cardinality categoricals; use basic hashing trick to numeric bins for fairness\n",
    "    df_iter = pd.read_csv(path_csv, chunksize=10_000_00)\n",
    "    df = next(df_iter)\n",
    "    if sample_n is not None and len(df) > sample_n:\n",
    "        df = df.sample(sample_n, random_state=42)\n",
    "    if 'click' in df.columns:\n",
    "        y = df['click'].astype(np.int32).values\n",
    "        X = df.drop(columns=['click'])\n",
    "    else:\n",
    "        # competition format: 'id','click',... ; fallback\n",
    "        y = df.iloc[:, 1].astype(np.int32).values\n",
    "        X = df.drop(columns=[df.columns[1]])\n",
    "    # Hash trick\n",
    "    MOD = 1_000_003\n",
    "    X_num = []\n",
    "    for col in X.columns:\n",
    "        if np.issubdtype(X[col].dtype, np.number):\n",
    "            X_num.append(X[col].astype(np.float64).values)\n",
    "        else:\n",
    "            X_num.append((X[col].astype(str).apply(hash).values % MOD).astype(np.float64))\n",
    "    X_num = np.vstack(X_num).T\n",
    "    return X_num, y\n",
    "\n",
    "def train_eval_one(model_name, model_ctor, X_train, y_train, X_test, y_test, is_multiclass=False, use_gpu=False):\n",
    "\n",
    "    if model_name == \"AdaptiveBayes\":\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        fit_stats = _measure_run(model_ctor['fit'], X_train_scaled, y_train)\n",
    "        pred_stats = _measure_run(model_ctor['predict'], X_test_scaled)\n",
    "\n",
    "    else:\n",
    "        # Обычное обучение для других моделей\n",
    "        fit_stats = _measure_run(model_ctor['fit'], X_train, y_train)\n",
    "        pred_stats = _measure_run(model_ctor['predict'], X_test)\n",
    "\n",
    "    y_pred = pred_stats[\"ret\"]\n",
    "    # Proba if available\n",
    "    auc = None\n",
    "    if not is_multiclass and 'predict_proba' in model_ctor:\n",
    "        proba_stats = _measure_run(model_ctor['predict_proba'], X_test)\n",
    "        y_prob = proba_stats[\"ret\"][:, 1] if y_prob_shape(proba_stats[\"ret\"]) else proba_stats[\"ret\"]\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "        proba_time = proba_stats[\"elapsed_s\"]\n",
    "    else:\n",
    "        proba_time = None\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"fit_s\": fit_stats[\"elapsed_s\"],\n",
    "        \"pred_s\": pred_stats[\"elapsed_s\"],\n",
    "        \"proba_s\": proba_time,\n",
    "        \"cpu_peak_mb_fit\": fit_stats[\"cpu_peak_mb\"],\n",
    "        \"gpu_mem_mb_fit\": fit_stats[\"gpu_mem_delta_mb\"],\n",
    "        \"acc\": acc,\n",
    "        \"auc\": auc\n",
    "    }\n",
    "\n",
    "def y_prob_shape(arr):\n",
    "    return (arr.ndim == 2) and (arr.shape[1] > 1)\n",
    "\n",
    "def make_models(use_gpu):\n",
    "    models = []\n",
    "\n",
    "    # AdaptiveBayes\n",
    "    ab = AdaptiveBayes(base_lr=1e-3, eps=1e-10, device='gpu' if use_gpu else 'cpu')\n",
    "    models.append((\n",
    "        \"AdaptiveBayes\",\n",
    "        {\n",
    "            \"fit\": ab.fit,\n",
    "            \"predict\": ab.predict,\n",
    "            \"predict_proba\": ab.predict_proba\n",
    "        }\n",
    "    ))\n",
    "\n",
    "    # XGBoost\n",
    "    if XGB_OK:\n",
    "        if use_gpu:\n",
    "            params = {\n",
    "                \"n_estimators\": 300,\n",
    "                \"max_depth\": 8,\n",
    "                \"learning_rate\": 0.1,\n",
    "                \"subsample\": 0.8,\n",
    "                \"colsample_bytree\": 0.8,\n",
    "                \"tree_method\": \"hist\",\n",
    "                \"device\": \"cuda\",\n",
    "                \"eval_metric\": \"auc\",\n",
    "            }\n",
    "        else:\n",
    "            params = {\n",
    "                \"n_estimators\": 300,\n",
    "                \"max_depth\": 8,\n",
    "                \"learning_rate\": 0.1,\n",
    "                \"subsample\": 0.8,\n",
    "                \"colsample_bytree\": 0.8,\n",
    "                \"tree_method\": \"hist\",\n",
    "                \"eval_metric\": \"auc\",\n",
    "            }\n",
    "\n",
    "        xgbc = xgb.XGBClassifier(**params)\n",
    "        models.append((\n",
    "            \"XGBoost\",\n",
    "            {\n",
    "                \"fit\": xgbc.fit,\n",
    "                \"predict\": xgbc.predict,\n",
    "                \"predict_proba\": xgbc.predict_proba\n",
    "            }\n",
    "        ))\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=300, n_jobs=-1, max_depth=None)\n",
    "    models.append((\n",
    "        \"RandomForest\",\n",
    "        {\n",
    "            \"fit\": rf.fit,\n",
    "            \"predict\": rf.predict,\n",
    "            \"predict_proba\": rf.predict_proba\n",
    "        }\n",
    "    ))\n",
    "\n",
    "    # Neural Net (sklearn MLP)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(256, 128), batch_size=512, max_iter=20, solver='adam', early_stopping=True, random_state=42)\n",
    "    models.append((\n",
    "        \"MLP\",\n",
    "        {\n",
    "            \"fit\": mlp.fit,\n",
    "            \"predict\": mlp.predict,\n",
    "            \"predict_proba\": mlp.predict_proba\n",
    "        }\n",
    "    ))\n",
    "\n",
    "    # LightGBM\n",
    "    if LGB_OK:\n",
    "        device_type = 'gpu' if use_gpu else 'cpu'\n",
    "\n",
    "        # Адаптивные параметры в зависимости от устройства\n",
    "        if device_type == 'gpu':\n",
    "            lgbm = lgb.LGBMClassifier(\n",
    "                n_estimators=300,          # Меньше итераций для GPU\n",
    "                num_leaves=511,            # Больше листьев\n",
    "                learning_rate=0.01,        # Меньше learning rate\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                device_type='gpu',\n",
    "                max_bin=127,               # Больше bins для GPU\n",
    "                min_data_in_leaf=100,      # Минимум данных в листе\n",
    "                min_gain_to_split=0.01,    # Минимальный gain для разбиения\n",
    "                verbose=-1                 # Убрать лишние warning'и\n",
    "            )\n",
    "        else:\n",
    "            lgbm = lgb.LGBMClassifier(\n",
    "                n_estimators=500,\n",
    "                num_leaves=255,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                device_type='cpu',\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "        models.append((\n",
    "            \"LightGBM\",\n",
    "            {\n",
    "                \"fit\": lgbm.fit,\n",
    "                \"predict\": lgbm.predict,\n",
    "                \"predict_proba\": lgbm.predict_proba\n",
    "            }\n",
    "        ))\n",
    "\n",
    "    # CatBoost\n",
    "    if CAT_OK:\n",
    "        cat = CatBoostClassifier(\n",
    "            iterations=500, depth=8, learning_rate=0.1, verbose=False,\n",
    "            task_type=\"GPU\" if use_gpu else \"CPU\"\n",
    "        )\n",
    "        models.append((\n",
    "            \"CatBoost\",\n",
    "            {\n",
    "                \"fit\": cat.fit,\n",
    "                \"predict\": cat.predict,\n",
    "                \"predict_proba\": cat.predict_proba\n",
    "            }\n",
    "        ))\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression(max_iter=200, solver='saga', n_jobs=-1)\n",
    "    models.append((\n",
    "        \"LogisticRegression\",\n",
    "        {\n",
    "            \"fit\": lr.fit,\n",
    "            \"predict\": lr.predict,\n",
    "            \"predict_proba\": lr.predict_proba\n",
    "        }\n",
    "    ))\n",
    "\n",
    "    return models\n",
    "\n",
    "def run_benchmark(datasets_config, use_gpu=False, test_size=0.2, val_size=0.0, output_csv=\"results.csv\"):\n",
    "    rows = []\n",
    "    for ds in datasets_config:\n",
    "        name = ds[\"name\"]\n",
    "        loader = ds[\"loader\"]\n",
    "        path = ds[\"path\"]\n",
    "        is_multiclass = ds.get(\"multiclass\", False)\n",
    "        sample_n = ds.get(\"sample_n\")\n",
    "        print(f\"Loading {name} ...\")\n",
    "        if name == \"Avazu\":\n",
    "            X, y = load_avazu(path, sample_n=sample_n)\n",
    "        else:\n",
    "            X, y = loader(path)\n",
    "            if sample_n is not None and len(X) > sample_n:\n",
    "                ridx = np.random.RandomState(42).choice(len(X), size=sample_n, replace=False)\n",
    "                X = X[ridx]\n",
    "                y = y[ridx]\n",
    "        print(f\"{name}: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y if not is_multiclass else None)\n",
    "        models = make_models(use_gpu=use_gpu)\n",
    "        for mname, m in models:\n",
    "            print(f\"Training {mname} on {name} ...\")\n",
    "            stats = train_eval_one(mname, m, X_tr, y_tr, X_te, y_te, is_multiclass=is_multiclass, use_gpu=use_gpu)\n",
    "            stats[\"dataset\"] = name\n",
    "            rows.append(stats)\n",
    "            print(stats)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved results to {output_csv}\")\n",
    "    return df\n",
    "\n",
    "def download_file(url, dest, chunk_size=2**20):\n",
    "    resp = requests.get(url, stream=True)\n",
    "    resp.raise_for_status()\n",
    "    with open(dest, 'wb') as f:\n",
    "        for chunk in resp.iter_content(chunk_size):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "def download_all_datasets(data_dir='data/'):\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # Credit Card Fraud (Figshare: альтернатива Kaggle)\n",
    "    creditcard_url = \"https://www.dropbox.com/s/b44o3t3ehmnx2b7/creditcard.csv?dl=1\"\n",
    "    creditcard_path = os.path.join(data_dir, \"creditcard.csv\")\n",
    "    if not os.path.exists(creditcard_path):\n",
    "        print(\"Downloading CreditCardFraud ...\")\n",
    "        download_file(creditcard_url, creditcard_path)\n",
    "\n",
    "    # HIGGS (UCI)\n",
    "    higgs_url = \"https://archive.ics.uci.edu/static/public/280/higgs.zip\"\n",
    "    higgs_zip = os.path.join(data_dir, \"higgs.zip\")\n",
    "    higgs_csv_gz = os.path.join(data_dir, \"HIGGS.csv.gz\")\n",
    "    if not os.path.exists(higgs_csv_gz):\n",
    "        print(\"Downloading HIGGS ...\")\n",
    "        download_file(higgs_url, higgs_zip)\n",
    "        with zipfile.ZipFile(higgs_zip) as zf:\n",
    "            zf.extract(\"HIGGS.csv.gz\", path=data_dir)\n",
    "        os.remove(higgs_zip)\n",
    "\n",
    "    # SUSY (UCI)\n",
    "    susy_url = \"https://archive.ics.uci.edu/static/public/279/susy.zip\"\n",
    "    susy_zip = os.path.join(data_dir, \"susy.zip\")\n",
    "    susy_csv_gz = os.path.join(data_dir, \"SUSY.csv.gz\")\n",
    "    if not os.path.exists(susy_csv_gz):\n",
    "        print(\"Downloading SUSY ...\")\n",
    "        download_file(susy_url, susy_zip)\n",
    "        with zipfile.ZipFile(susy_zip) as zf:\n",
    "            zf.extract(\"SUSY.csv.gz\", path=data_dir)\n",
    "        os.remove(susy_zip)\n",
    "\n",
    "    # KDDCup99 (10 percent) (UCI)\n",
    "    kdd_url = \"https://figshare.com/ndownloader/files/5976042\"\n",
    "    kdd_gz = os.path.join(data_dir, \"kddcup.data_10_percent.gz\")\n",
    "    kdd_csv = os.path.join(data_dir, \"kddcup.data_10_percent.csv\")\n",
    "    if not os.path.exists(kdd_csv):\n",
    "        print(\"Downloading KDDCup99 ...\")\n",
    "        download_file(kdd_url, kdd_gz)\n",
    "        import gzip\n",
    "        with gzip.open(kdd_gz, 'rb') as f_in, open(kdd_csv, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "        os.remove(kdd_gz)\n",
    "\n",
    "    # Covertype (UCI/sklearn, already CSV)\n",
    "    covertype_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\n",
    "    covertype_gz = os.path.join(data_dir, \"covtype.data.gz\")\n",
    "    covertype_csv = os.path.join(data_dir, \"covtype.csv\")\n",
    "    if not os.path.exists(covertype_csv):\n",
    "        print(\"Downloading Covertype ...\")\n",
    "        download_file(covertype_url, covertype_gz)\n",
    "        import gzip\n",
    "        with gzip.open(covertype_gz, 'rb') as f_in, open(covertype_csv, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "        os.remove(covertype_gz)\n",
    "\n",
    "    # HEPMASS\n",
    "    hepmass_url = \"https://archive.ics.uci.edu/static/public/347/hepmass.zip\"\n",
    "    hepmass_csv = os.path.join(data_dir, \"HEPMASS_train.csv\")\n",
    "    if not os.path.exists(hepmass_csv):\n",
    "        print(\"Downloading HEPMASS ...\")\n",
    "        download_file(hepmass_url, hepmass_csv)\n",
    "\n",
    "    # Avazu CTR (HF mirror, 2m строк фрагмент — fastest for dev)\n",
    "    avazu_url = \"https://www.kaggle.com/api/v1/datasets/download/wuyingwen06/avazu-ctr-train\"\n",
    "    avazu_csv = os.path.join(data_dir, \"avazu-ctr-train.zip\")\n",
    "    if not os.path.exists(avazu_csv):\n",
    "        print(\"Downloading Avazu...\")\n",
    "        download_file(avazu_url, avazu_csv)\n",
    "\n",
    "    print(\"Done downloading all datasets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMZOBvzvqVAz",
    "outputId": "58a91e00-481f-4af8-cac4-8e4f55ef87ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading HEPMASS ...\n"
     ]
    }
   ],
   "source": [
    "data_dir='data/'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "# hepmass_url = \"https://www.openml.org/data/get_csv/2419/BNG_balance-scale.csv\"  # Временная замена\n",
    "# hepmass_csv = os.path.join(data_dir, \"HEPMASS_train.csv\")\n",
    "# if not os.path.exists(hepmass_csv):\n",
    "#     print(\"Downloading HEPMASS (alternative dataset)...\")\n",
    "#     try:\n",
    "#         download_file(hepmass_url, hepmass_csv)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to download HEPMASS: {e}\")\n",
    "#         # Создать заглушку, чтобы не прерывать весь benchmark\n",
    "#         print(\"Creating dummy HEPMASS dataset...\")\n",
    "#         np.random.seed(42)\n",
    "#         X_dummy = np.random.randn(10000, 28).astype(np.float64)\n",
    "#         y_dummy = np.random.randint(0, 2, 10000).astype(np.int32)\n",
    "#         dummy_df = pd.DataFrame(X_dummy)\n",
    "#         dummy_df['label'] = y_dummy\n",
    "#         dummy_df.to_csv(hepmass_csv, index=False)\n",
    "hepmass_url = \"https://archive.ics.uci.edu/static/public/347/hepmass.zip\"\n",
    "hepmass_csv = os.path.join(data_dir, \"HEPMASS_train.csv\")\n",
    "if not os.path.exists(hepmass_csv):\n",
    "    print(\"Downloading HEPMASS ...\")\n",
    "    download_file(hepmass_url, hepmass_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY4D39VIxEHL"
   },
   "source": [
    "# Main body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4AZBu4UVyqId",
    "outputId": "de8fc492-f11d-4001-96e6-3f8dab2732ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Avazu...\n",
      "Done downloading all datasets.\n"
     ]
    }
   ],
   "source": [
    "download_all_datasets(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Qr4XnBnzzk0V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wdW0_KHowy3a",
    "outputId": "8f75d957-691f-4844-99ec-d82e459710df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CreditCardFraud ...\n",
      "CreditCardFraud: X=(284807, 30), y=(284807,)\n",
      "Training AdaptiveBayes on CreditCardFraud ...\n",
      "{'model': 'AdaptiveBayes', 'fit_s': 0.4015603170009854, 'pred_s': 0.38212611400012975, 'proba_s': 0.378300954002043, 'cpu_peak_mb_fit': 0.0, 'gpu_mem_mb_fit': 0.0, 'acc': 0.3021136898283066, 'auc': np.float64(0.925451291588667), 'dataset': 'CreditCardFraud'}\n",
      "Training XGBoost on CreditCardFraud ...\n",
      "{'model': 'XGBoost', 'fit_s': 1.3464879339990148, 'pred_s': 0.4116181350000261, 'proba_s': 0.4156731770017359, 'cpu_peak_mb_fit': 2.46875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9995435553526912, 'auc': np.float64(0.9702255937546657), 'dataset': 'CreditCardFraud'}\n",
      "Training RandomForest on CreditCardFraud ...\n",
      "{'model': 'RandomForest', 'fit_s': 116.577296681, 'pred_s': 0.5763333159993635, 'proba_s': 0.5700973350030836, 'cpu_peak_mb_fit': 15.10546875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9995962220427653, 'auc': np.float64(0.9515624641105739), 'dataset': 'CreditCardFraud'}\n",
      "Training MLP on CreditCardFraud ...\n",
      "{'model': 'MLP', 'fit_s': 33.54179190499781, 'pred_s': 1.2574465719990258, 'proba_s': 1.5962102439989394, 'cpu_peak_mb_fit': 0.0546875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9982795547909132, 'auc': np.float64(0.5045023285059663), 'dataset': 'CreditCardFraud'}\n",
      "Training LightGBM on CreditCardFraud ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'LightGBM', 'fit_s': 3.962053786002798, 'pred_s': 0.7214813649989082, 'proba_s': 0.6976918760010449, 'cpu_peak_mb_fit': 76.40234375, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9996137776061234, 'auc': np.float64(0.9643131696966912), 'dataset': 'CreditCardFraud'}\n",
      "Training LogisticRegression on CreditCardFraud ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'LogisticRegression', 'fit_s': 36.51700981199974, 'pred_s': 0.4217017090013542, 'proba_s': 0.43432605099951616, 'cpu_peak_mb_fit': 0.13671875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9982795547909132, 'auc': np.float64(0.6287712608960297), 'dataset': 'CreditCardFraud'}\n",
      "Loading HIGGS ...\n",
      "HIGGS: X=(2000000, 28), y=(2000000,)\n",
      "Training AdaptiveBayes on HIGGS ...\n",
      "{'model': 'AdaptiveBayes', 'fit_s': 0.5226041179994354, 'pred_s': 0.41296176399919204, 'proba_s': 0.4164324619996478, 'cpu_peak_mb_fit': 0.0, 'gpu_mem_mb_fit': 0.0, 'acc': 0.5853925, 'auc': np.float64(0.6454456960487918), 'dataset': 'HIGGS'}\n",
      "Training XGBoost on HIGGS ...\n",
      "{'model': 'XGBoost', 'fit_s': 5.5433791169998585, 'pred_s': 0.5620416429992474, 'proba_s': 0.5638459370020428, 'cpu_peak_mb_fit': 2.21875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.75044, 'auc': np.float64(0.8333018787578815), 'dataset': 'HIGGS'}\n",
      "Training RandomForest on HIGGS ...\n",
      "{'model': 'RandomForest', 'fit_s': 797.721280316, 'pred_s': 10.282766372998594, 'proba_s': 10.177214924999134, 'cpu_peak_mb_fit': 5992.54296875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.743805, 'auc': np.float64(0.825493361568764), 'dataset': 'HIGGS'}\n",
      "Training MLP on HIGGS ...\n"
     ]
    }
   ],
   "source": [
    "# Example configuration; update paths to local files\n",
    "datasets = [\n",
    "    {\"name\": \"CreditCardFraud\", \"loader\": load_creditcard_fraud, \"path\": \"data/creditcard.csv\", \"multiclass\": False},\n",
    "    {\"name\": \"HIGGS\", \"loader\": load_higgs, \"path\": \"data/HIGGS.csv.gz\", \"multiclass\": False, \"sample_n\": 2_000_000},\n",
    "    {\"name\": \"SUSY\", \"loader\": load_susy, \"path\": \"data/SUSY.csv.gz\", \"multiclass\": False, \"sample_n\": 2_000_000},\n",
    "    {\"name\": \"KDDCup99\", \"loader\": load_kddcup99, \"path\": \"data/kddcup.data_10_percent.csv\", \"multiclass\": False},\n",
    "    {\"name\": \"Covertype\", \"loader\": load_covertype, \"path\": \"data/covtype.csv\", \"multiclass\": True},\n",
    "    {\"name\": \"HEPMASS\", \"loader\": create_synthetic_hepmass, \"path\": \"data/HEPMASS_train.csv\", \"multiclass\": False},\n",
    "    {\"name\": \"Avazu\", \"loader\": load_avazu, \"path\": \"data/avazu-ctr-train.zip\", \"multiclass\": False, \"sample_n\": 2_000_000},\n",
    "]\n",
    "use_gpu = GPU_OK\n",
    "run_benchmark(datasets, use_gpu=use_gpu, output_csv=\"benchmark_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "An0GVGGqwy9g"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}