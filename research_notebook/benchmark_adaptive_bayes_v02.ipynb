{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0eda1f5b72ec472fbcafcd50af9c29b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51b2ea1a6c5b4ee293d0bbfd6f359a58",
              "IPY_MODEL_f0b3d4764f3c4abcac81b16e7b60eb6c",
              "IPY_MODEL_1abb012ac31e4fb989870cf2af69d87e"
            ],
            "layout": "IPY_MODEL_e9e9911dca7043268cb41d976be01e1b"
          }
        },
        "727a82b7eeae4aad96eedc1cad1c2902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91dcad525bab496ab8aacc32bab897b6",
              "IPY_MODEL_cb519d3358d14ecfa05a8d755408db9a",
              "IPY_MODEL_52586f3269a7454fb0cf360416cdd742"
            ],
            "layout": "IPY_MODEL_f513141b5e9c4d049ea59fd56987c4e0"
          }
        },
        "dd74db8bbda34a72abad0c59bda97b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ce0902c2ad749f3a55e086fb5395ae9",
              "IPY_MODEL_603c975075994e42a933a241910870d5",
              "IPY_MODEL_2ecf146da98c40dd86bade74c049bce7"
            ],
            "layout": "IPY_MODEL_b7c2e0573be34062b0a307b46dc8968f"
          }
        },
        "ee1a1ceb09784e7fad8aa5297c4e6d0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a84b10cb90a472fa1f046887b8af884",
              "IPY_MODEL_9e049e9470934e8396014caf56fd80b0",
              "IPY_MODEL_062e968cb3cf4c7dbe80e257f4a16cc6"
            ],
            "layout": "IPY_MODEL_72bebdc79b884c32bd4423849177993b"
          }
        },
        "5999d83209444c2994430ea13b32071f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9e08d76548f4a0185a192aa56d5a0a3",
              "IPY_MODEL_d56b2f72fe2c45d991460c97cc737915",
              "IPY_MODEL_2831a967073e4c4381308363b112a973"
            ],
            "layout": "IPY_MODEL_2965355fddfb48ae88881a4ddf3197f0"
          }
        },
        "e5114ee1c15c4e128cd72dcc8d0db4c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c9ba410ff804da691d997f044364003",
              "IPY_MODEL_426eb09e48e2420c95dcf9a4c5099b13",
              "IPY_MODEL_dfb13e75c790419fa3c6b36c936ba011"
            ],
            "layout": "IPY_MODEL_bac80ec77eab4415b6bf891a39130434"
          }
        },
        "d17e0c4dd34f40ab8f65e8eb8b7751ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b106766f073a405aab81d7fc470fd2ec",
              "IPY_MODEL_4fdb5e945e644af9b4d66362ea5a63b5",
              "IPY_MODEL_3504d21273f04addb0b5c86b93adbb29"
            ],
            "layout": "IPY_MODEL_d4f4f356d5874c3b9639c7875eeaa567"
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install memory_profiler optuna"
      ],
      "metadata": {
        "id": "3abXouJZxXNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "Z6anN_kdw2rS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymWQbsSvww8t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import gzip\n",
        "import math\n",
        "import json\n",
        "import shutil\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from pandas.errors import ParserError\n",
        "\n",
        "# Optional GPU memory\n",
        "try:\n",
        "    import cupy as cp\n",
        "    from cupy.cuda import runtime as cuda_rt\n",
        "    GPU_OK = True\n",
        "except Exception:\n",
        "    GPU_OK = False\n",
        "\n",
        "import psutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from memory_profiler import memory_usage\n",
        "\n",
        "# Baselines\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGB_OK = True\n",
        "except Exception:\n",
        "    XGB_OK = False\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LGB_OK = True\n",
        "except Exception:\n",
        "    LGB_OK = False\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "    CAT_OK = True\n",
        "except Exception:\n",
        "    CAT_OK = False\n",
        "\n",
        "# from adaptive_bayes import AdaptiveBayes\n",
        "from improved_adaptive_bayes import AdaptiveBayes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "9u3m4qcUxA6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _gpu_mem_info():\n",
        "    if not GPU_OK:\n",
        "        return None, None\n",
        "    free_b, total_b = cuda_rt.memGetInfo()\n",
        "    return free_b, total_b\n",
        "\n",
        "def _proc_rss_mb():\n",
        "    return psutil.Process(os.getpid()).memory_info().rss / (1024*1024)\n",
        "\n",
        "def _measure_run(fn, *args, **kwargs):\n",
        "    # CPU mem before\n",
        "    rss_before = _proc_rss_mb()\n",
        "    # GPU mem before\n",
        "    free0, total0 = _gpu_mem_info()\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    def _wrapped():\n",
        "        return fn(*args, **kwargs)\n",
        "\n",
        "    mem_trace = memory_usage((_wrapped, (), {}), max_iterations=1, interval=0.1, retval=True)\n",
        "    if isinstance(mem_trace, tuple) and len(mem_trace) == 2:\n",
        "        mem_series, ret = mem_trace\n",
        "    else:\n",
        "        mem_series, ret = mem_trace, None\n",
        "\n",
        "    elapsed = time.perf_counter() - t0\n",
        "    rss_after = _proc_rss_mb()\n",
        "    free1, total1 = _gpu_mem_info()\n",
        "\n",
        "    peak_cpu = max(mem_series) - rss_before if mem_series else 0.0\n",
        "    rss_delta = rss_after - rss_before\n",
        "\n",
        "    gpu_delta = None\n",
        "    if free0 is not None and free1 is not None:\n",
        "        gpu_delta = (free0 - free1) / (1024*1024)\n",
        "\n",
        "    return {\n",
        "        \"elapsed_s\": elapsed,\n",
        "        \"cpu_rss_mb_before\": rss_before,\n",
        "        \"cpu_rss_mb_after\": rss_after,\n",
        "        \"cpu_rss_delta_mb\": rss_delta,\n",
        "        \"cpu_peak_mb\": peak_cpu,\n",
        "        \"gpu_mem_delta_mb\": gpu_delta,\n",
        "        \"ret\": ret\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset loaders (paths expected)\n",
        "# ---------------------------\n",
        "def load_creditcard_fraud(path_csv):\n",
        "    # Kaggle: V1..V28 + Time, Amount, Class; binary Class\n",
        "    df = pd.read_csv(path_csv)\n",
        "    y = df['Class'].astype(np.int32).values\n",
        "    X = df.drop(columns=['Class']).values.astype(np.float64)\n",
        "    return X, y\n",
        "\n",
        "def load_higgs(path_gz):\n",
        "    # UCI: CSV.gz with label first, then 28 features\n",
        "    with gzip.open(path_gz, 'rt') as f:\n",
        "        df = pd.read_csv(f, header=None)\n",
        "    y = df.iloc[:, 0].astype(np.int32).values\n",
        "    X = df.iloc[:, 1:].values.astype(np.float64)\n",
        "    return X, y\n",
        "\n",
        "def load_susy(path_gz):\n",
        "    with gzip.open(path_gz, 'rt') as f:\n",
        "        df = pd.read_csv(f, header=None)\n",
        "    y = df.iloc[:, 0].astype(np.int32).values\n",
        "    X = df.iloc[:, 1:].values.astype(np.float64)\n",
        "    return X, y\n",
        "\n",
        "def load_kddcup99(path_csv, drop_cats=True):\n",
        "    # Mixed dtypes; simplify to numeric by one-hot or drop_cats\n",
        "    df = pd.read_csv(path_csv, header=None)\n",
        "    if drop_cats:\n",
        "        # Keep numeric columns only\n",
        "        num_df = df.select_dtypes(include=[np.number])\n",
        "        # Target can be last column or named; assume last is label string -> map to binary (normal vs attack)\n",
        "        # If last column non-numeric, we map\n",
        "        if not np.issubdtype(df.iloc[:, -1].dtype, np.number):\n",
        "            y = (df.iloc[:, -1].astype(str) != 'normal.').astype(np.int32).values\n",
        "        else:\n",
        "            y = df.iloc[:, -1].astype(np.int32).values\n",
        "        X = num_df.iloc[:, :-1].values.astype(np.float64)\n",
        "    else:\n",
        "        # One-hot encode categoricals\n",
        "        y = (df.iloc[:, -1].astype(str) != 'normal.').astype(np.int32).values\n",
        "        X = pd.get_dummies(df.iloc[:, :-1]).values.astype(np.float64)\n",
        "    return X, y\n",
        "\n",
        "def load_covertype(path_csv):\n",
        "    df = pd.read_csv(path_csv)\n",
        "    target_col = 'Cover_Type' if 'Cover_Type' in df.columns else df.columns[-1]\n",
        "    y = np.asarray(df[target_col], dtype=np.int32) - 1\n",
        "    # Convert to binary: class1 vs others to align with AUC, or keep multiclass for accuracy\n",
        "    # Here we keep multiclass; AUC will be skipped for multiclass\n",
        "    X = df.drop(columns=[target_col]).values.astype(np.float64)\n",
        "    return X, y\n",
        "\n",
        "def create_synthetic_hepmass():\n",
        "    \"\"\"Создает синтетический датасет в стиле HEPMASS\"\"\"\n",
        "    print(\"Creating synthetic HEPMASS-like dataset...\")\n",
        "    np.random.seed(42)\n",
        "    n_samples = 500000\n",
        "    n_features = 28\n",
        "\n",
        "    # Создать корреляционные признаки как в физических данных\n",
        "    X = np.random.randn(n_samples, n_features).astype(np.float64)\n",
        "\n",
        "    # Добавить нелинейные взаимодействия для реализма\n",
        "    X[:, 1] = X[:, 0] ** 2 + 0.5 * np.random.randn(n_samples)\n",
        "    X[:, 2] = X[:, 0] * X[:, 1] + 0.3 * np.random.randn(n_samples)\n",
        "\n",
        "    # Создать сложную целевую переменную\n",
        "    signal = (0.3 * X[:, 0] + 0.2 * X[:, 1] - 0.1 * X[:, 2] +\n",
        "              0.15 * X[:, 3] * X[:, 4] + 0.1 * np.sin(X[:, 5]))\n",
        "    noise = 0.5 * np.random.randn(n_samples)\n",
        "    y = (signal + noise > 0).astype(np.int32)\n",
        "\n",
        "    print(f\"Synthetic dataset: X={X.shape}, y={y.shape}, class balance={np.mean(y):.3f}\")\n",
        "    return X, y\n",
        "\n",
        "def load_hepmass(path_csv):\n",
        "    \"\"\"Робастная загрузка HEPMASS с обработкой ошибок парсинга\"\"\"\n",
        "    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
        "    df = None\n",
        "\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            # Попробовать разные варианты парсинга\n",
        "            parsing_options = [\n",
        "                # Стандартный CSV\n",
        "                {'encoding': encoding, 'sep': ','},\n",
        "                # Разделитель - пробел/табуляция\n",
        "                {'encoding': encoding, 'sep': r'\\s+', 'engine': 'python'},\n",
        "                # Пропуск плохих строк\n",
        "                {'encoding': encoding, 'sep': ',', 'on_bad_lines': 'skip'},\n",
        "                # Без заголовка\n",
        "                {'encoding': encoding, 'sep': ',', 'header': None, 'on_bad_lines': 'skip'},\n",
        "            ]\n",
        "\n",
        "            for options in parsing_options:\n",
        "                try:\n",
        "                    df = pd.read_csv(path_csv, **options)\n",
        "                    print(f\"Successfully loaded HEPMASS with encoding: {encoding}, options: {options}\")\n",
        "                    break\n",
        "                except (ParserError, pd.errors.ParserError):\n",
        "                    continue\n",
        "\n",
        "            if df is not None:\n",
        "                break\n",
        "\n",
        "        except UnicodeDecodeError:\n",
        "            continue\n",
        "\n",
        "    if df is None or df.empty:\n",
        "        print(\"Failed to load HEPMASS, creating synthetic dataset...\")\n",
        "        return create_synthetic_hepmass()\n",
        "\n",
        "    # Обработка разных форматов колонок\n",
        "    print(f\"HEPMASS loaded: {df.shape}, columns: {list(df.columns)}\")\n",
        "\n",
        "    # Попробовать найти целевую переменную\n",
        "    if '# label' in df.columns:\n",
        "        y = df['# label'].astype(np.int32)\n",
        "        X = df.drop(columns=['# label']).select_dtypes(include=[np.number]).values.astype(np.float64)\n",
        "    elif 'type' in df.columns:\n",
        "        y = df['type'].astype(np.int32)\n",
        "        X = df.drop(columns=['type']).select_dtypes(include=[np.number]).values.astype(np.float64)\n",
        "    elif 'label' in df.columns:\n",
        "        y = df['label'].astype(np.int32)\n",
        "        X = df.drop(columns=['label']).select_dtypes(include=[np.number]).values.astype(np.float64)\n",
        "    else:\n",
        "        # Предполагаем первую или последнюю колонку как target\n",
        "        if df.shape[1] > 1:\n",
        "            # Попробовать последнюю колонку как target\n",
        "            last_col = df.iloc[:, -1]\n",
        "            if last_col.dtype in ['int64', 'float64'] and last_col.nunique() <= 10:\n",
        "                y = last_col.astype(np.int32)\n",
        "                X = df.iloc[:, :-1].select_dtypes(include=[np.number]).values.astype(np.float64)\n",
        "            else:\n",
        "                # Первая колонка как target\n",
        "                y = df.iloc[:, 0].astype(np.int32)\n",
        "                X = df.iloc[:, 1:].select_dtypes(include=[np.number]).values.astype(np.float64)\n",
        "        else:\n",
        "            print(\"Cannot determine target variable, creating synthetic...\")\n",
        "            return create_synthetic_hepmass()\n",
        "\n",
        "    # Проверки корректности\n",
        "    if X.shape[0] == 0 or X.shape[1] == 0:\n",
        "        print(\"Empty feature matrix, creating synthetic...\")\n",
        "        return create_synthetic_hepmass()\n",
        "\n",
        "    # Конвертация меток в бинарные если нужно\n",
        "    if len(np.unique(y)) > 2:\n",
        "        print(f\"Converting {len(np.unique(y))} classes to binary\")\n",
        "        y = (y > np.median(y)).astype(np.int32)\n",
        "\n",
        "    # Убрать NaN/inf значения\n",
        "    X = np.nan_to_num(X, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def load_avazu(path_csv, sample_n=None):\n",
        "    # High-cardinality categoricals; use basic hashing trick to numeric bins for fairness\n",
        "    df_iter = pd.read_csv(path_csv, chunksize=10_000_00)\n",
        "    df = next(df_iter)\n",
        "    if sample_n is not None and len(df) > sample_n:\n",
        "        df = df.sample(sample_n, random_state=42)\n",
        "    if 'click' in df.columns:\n",
        "        y = df['click'].astype(np.int32).values\n",
        "        X = df.drop(columns=['click'])\n",
        "    else:\n",
        "        # competition format: 'id','click',... ; fallback\n",
        "        y = df.iloc[:, 1].astype(np.int32).values\n",
        "        X = df.drop(columns=[df.columns[1]])\n",
        "    # Hash trick\n",
        "    MOD = 1_000_003\n",
        "    X_num = []\n",
        "    for col in X.columns:\n",
        "        if np.issubdtype(X[col].dtype, np.number):\n",
        "            X_num.append(X[col].astype(np.float64).values)\n",
        "        else:\n",
        "            X_num.append((X[col].astype(str).apply(hash).values % MOD).astype(np.float64))\n",
        "    X_num = np.vstack(X_num).T\n",
        "    return X_num, y\n",
        "\n",
        "def train_eval_one_old(model_name, model_ctor, X_train, y_train, X_test, y_test, is_multiclass=False, use_gpu=False):\n",
        "\n",
        "    if model_name == \"AdaptiveBayes\":\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        fit_stats = _measure_run(model_ctor['fit'], X_train_scaled, y_train)\n",
        "        pred_stats = _measure_run(model_ctor['predict'], X_test_scaled)\n",
        "    elif model_name == \"XGBoost\":\n",
        "        X_train_gpu = cp.asarray(X_train)\n",
        "        y_train_gpu = cp.asarray(y_train)\n",
        "        X_test_gpu = cp.asarray(X_test)\n",
        "        y_test = cp.asarray(y_test)\n",
        "        fit_stats = _measure_run(model_ctor['fit'], X_train_gpu, y_train_gpu)\n",
        "        pred_stats = _measure_run(model_ctor['predict'], X_test_gpu)\n",
        "    else:\n",
        "        # Обычное обучение для других моделей\n",
        "        fit_stats = _measure_run(model_ctor['fit'], X_train, y_train)\n",
        "        pred_stats = _measure_run(model_ctor['predict'], X_test)\n",
        "\n",
        "    y_pred = pred_stats[\"ret\"]\n",
        "    # Proba if available\n",
        "    auc = None\n",
        "    if not is_multiclass and 'predict_proba' in model_ctor:\n",
        "        proba_stats = _measure_run(model_ctor['predict_proba'], X_test)\n",
        "        y_prob = proba_stats[\"ret\"][:, 1] if y_prob_shape(proba_stats[\"ret\"]) else proba_stats[\"ret\"]\n",
        "        auc = roc_auc_score(y_test, y_prob)\n",
        "        proba_time = proba_stats[\"elapsed_s\"]\n",
        "    else:\n",
        "        proba_time = None\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    return {\n",
        "        \"model\": model_name,\n",
        "        \"fit_s\": fit_stats[\"elapsed_s\"],\n",
        "        \"pred_s\": pred_stats[\"elapsed_s\"],\n",
        "        \"proba_s\": proba_time,\n",
        "        \"cpu_peak_mb_fit\": fit_stats[\"cpu_peak_mb\"],\n",
        "        \"gpu_mem_mb_fit\": fit_stats[\"gpu_mem_delta_mb\"],\n",
        "        \"acc\": acc,\n",
        "        \"auc\": auc\n",
        "    }\n",
        "\n",
        "def train_eval_one(model_name, model_ctor, X_train, y_train, X_test, y_test, is_multiclass=False, use_gpu=False):\n",
        "    # ДОБАВИТЬ: преобразование y_test в NumPy для sklearn метрик\n",
        "    if hasattr(y_test, 'get'):  # CuPy массив\n",
        "        y_test_np = y_test.get()\n",
        "    else:\n",
        "        y_test_np = np.asarray(y_test)\n",
        "\n",
        "    # Нормализация для AdaptiveBayes\n",
        "    if model_name == \"AdaptiveBayes\":\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        fit_stats = _measure_run(model_ctor['fit'], X_train_scaled, y_train)\n",
        "        pred_stats = _measure_run(model_ctor['predict'], X_test_scaled)\n",
        "        y_pred = pred_stats[\"ret\"]\n",
        "        if not is_multiclass and 'predict_proba' in model_ctor:\n",
        "            proba_stats = _measure_run(model_ctor['predict_proba'], X_test_scaled)\n",
        "            y_prob = proba_stats[\"ret\"]\n",
        "            # y_prob уже NumPy из improved_adaptive_bayes.py\n",
        "            auc = roc_auc_score(y_test_np, y_prob)  # Использовать y_test_np\n",
        "            proba_time = proba_stats[\"elapsed_s\"]\n",
        "        else:\n",
        "            proba_time = None\n",
        "            auc = None\n",
        "    else:\n",
        "        # Обычное обучение для других моделей\n",
        "        fit_stats = _measure_run(model_ctor['fit'], X_train, y_train)\n",
        "        pred_stats = _measure_run(model_ctor['predict'], X_test)\n",
        "        y_pred = pred_stats[\"ret\"]\n",
        "        if not is_multiclass and 'predict_proba' in model_ctor:\n",
        "            proba_stats = _measure_run(model_ctor['predict_proba'], X_test)\n",
        "            y_prob = proba_stats[\"ret\"][:, 1] if y_prob_shape(proba_stats[\"ret\"]) else proba_stats[\"ret\"]\n",
        "            auc = roc_auc_score(y_test_np, y_prob)  # Использовать y_test_np\n",
        "            proba_time = proba_stats[\"elapsed_s\"]\n",
        "        else:\n",
        "            proba_time = None\n",
        "            auc = None\n",
        "\n",
        "    # Преобразовать y_pred в NumPy если нужно\n",
        "    if hasattr(y_pred, 'get'):\n",
        "        y_pred = y_pred.get()\n",
        "\n",
        "    acc = accuracy_score(y_test_np, y_pred)  # Использовать y_test_np\n",
        "\n",
        "    return {\n",
        "        \"model\": model_name,\n",
        "        \"fit_s\": fit_stats[\"elapsed_s\"],\n",
        "        \"pred_s\": pred_stats[\"elapsed_s\"],\n",
        "        \"proba_s\": proba_time,\n",
        "        \"cpu_peak_mb_fit\": fit_stats[\"cpu_peak_mb\"],\n",
        "        \"gpu_mem_mb_fit\": fit_stats[\"gpu_mem_delta_mb\"],\n",
        "        \"acc\": acc,\n",
        "        \"auc\": auc\n",
        "    }\n",
        "\n",
        "def y_prob_shape(arr):\n",
        "    return (arr.ndim == 2) and (arr.shape[1] > 1)\n",
        "\n",
        "def make_models(use_gpu):\n",
        "    models = []\n",
        "\n",
        "    # AdaptiveBayes\n",
        "    ab = AdaptiveBayes(base_lr=1e-3, eps=1e-10, device='gpu' if use_gpu else 'cpu')\n",
        "    models.append((\n",
        "        \"AdaptiveBayes\",\n",
        "        {\n",
        "            \"fit\": ab.fit,\n",
        "            \"predict\": ab.predict,\n",
        "            \"predict_proba\": ab.predict_proba\n",
        "        }\n",
        "    ))\n",
        "\n",
        "    # XGBoost\n",
        "    if XGB_OK:\n",
        "        if use_gpu:\n",
        "            params = {\n",
        "                \"n_estimators\": 300,\n",
        "                \"max_depth\": 8,\n",
        "                \"learning_rate\": 0.1,\n",
        "                \"subsample\": 0.8,\n",
        "                \"colsample_bytree\": 0.8,\n",
        "                \"tree_method\": \"hist\",\n",
        "                \"device\": \"cuda\",\n",
        "                \"eval_metric\": \"auc\",\n",
        "            }\n",
        "        else:\n",
        "            params = {\n",
        "                \"n_estimators\": 300,\n",
        "                \"max_depth\": 8,\n",
        "                \"learning_rate\": 0.1,\n",
        "                \"subsample\": 0.8,\n",
        "                \"colsample_bytree\": 0.8,\n",
        "                \"tree_method\": \"hist\",\n",
        "                \"eval_metric\": \"auc\",\n",
        "            }\n",
        "\n",
        "        xgbc = xgb.XGBClassifier(**params)\n",
        "        models.append((\n",
        "            \"XGBoost\",\n",
        "            {\n",
        "                \"fit\": xgbc.fit,\n",
        "                \"predict\": xgbc.predict,\n",
        "                \"predict_proba\": xgbc.predict_proba\n",
        "            }\n",
        "        ))\n",
        "\n",
        "    # Random Forest\n",
        "    rf = RandomForestClassifier(n_estimators=300, n_jobs=-1, max_depth=None)\n",
        "    models.append((\n",
        "        \"RandomForest\",\n",
        "        {\n",
        "            \"fit\": rf.fit,\n",
        "            \"predict\": rf.predict,\n",
        "            \"predict_proba\": rf.predict_proba\n",
        "        }\n",
        "    ))\n",
        "\n",
        "    # Neural Net (sklearn MLP)\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=(256, 128), batch_size=512, max_iter=20, solver='adam', early_stopping=True, random_state=42)\n",
        "    models.append((\n",
        "        \"MLP\",\n",
        "        {\n",
        "            \"fit\": mlp.fit,\n",
        "            \"predict\": mlp.predict,\n",
        "            \"predict_proba\": mlp.predict_proba\n",
        "        }\n",
        "    ))\n",
        "\n",
        "    # LightGBM\n",
        "    if LGB_OK:\n",
        "        device_type = 'gpu' if use_gpu else 'cpu'\n",
        "\n",
        "        # Адаптивные параметры в зависимости от устройства\n",
        "        if device_type == 'gpu':\n",
        "            lgbm = lgb.LGBMClassifier(\n",
        "                n_estimators=300,          # Меньше итераций для GPU\n",
        "                num_leaves=511,            # Больше листьев\n",
        "                learning_rate=0.01,        # Меньше learning rate\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                device_type='gpu',\n",
        "                max_bin=127,               # Больше bins для GPU\n",
        "                min_data_in_leaf=100,      # Минимум данных в листе\n",
        "                min_gain_to_split=0.01,    # Минимальный gain для разбиения\n",
        "                verbose=-1                 # Убрать лишние warning'и\n",
        "            )\n",
        "        else:\n",
        "            lgbm = lgb.LGBMClassifier(\n",
        "                n_estimators=500,\n",
        "                num_leaves=255,\n",
        "                learning_rate=0.05,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                device_type='cpu',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "        models.append((\n",
        "            \"LightGBM\",\n",
        "            {\n",
        "                \"fit\": lgbm.fit,\n",
        "                \"predict\": lgbm.predict,\n",
        "                \"predict_proba\": lgbm.predict_proba\n",
        "            }\n",
        "        ))\n",
        "\n",
        "    # CatBoost\n",
        "    if CAT_OK:\n",
        "        cat = CatBoostClassifier(\n",
        "            iterations=500, depth=8, learning_rate=0.1, verbose=False,\n",
        "            task_type=\"GPU\" if use_gpu else \"CPU\"\n",
        "        )\n",
        "        models.append((\n",
        "            \"CatBoost\",\n",
        "            {\n",
        "                \"fit\": cat.fit,\n",
        "                \"predict\": cat.predict,\n",
        "                \"predict_proba\": cat.predict_proba\n",
        "            }\n",
        "        ))\n",
        "\n",
        "    # Logistic Regression\n",
        "    lr = LogisticRegression(max_iter=200, solver='saga', n_jobs=-1)\n",
        "    models.append((\n",
        "        \"LogisticRegression\",\n",
        "        {\n",
        "            \"fit\": lr.fit,\n",
        "            \"predict\": lr.predict,\n",
        "            \"predict_proba\": lr.predict_proba\n",
        "        }\n",
        "    ))\n",
        "\n",
        "    return models\n",
        "\n",
        "def run_benchmark(datasets_config, use_gpu=False, test_size=0.2, val_size=0.0, output_csv=\"results.csv\"):\n",
        "    rows = []\n",
        "    for ds in datasets_config:\n",
        "        name = ds[\"name\"]\n",
        "        loader = ds[\"loader\"]\n",
        "        path = ds[\"path\"]\n",
        "        is_multiclass = ds.get(\"multiclass\", False)\n",
        "        sample_n = ds.get(\"sample_n\")\n",
        "        print(f\"Loading {name} ...\")\n",
        "        if name == \"Synthetic\":\n",
        "            X, y = create_synthetic_hepmass()\n",
        "        elif name == \"Avazu\":\n",
        "            X, y = load_avazu(path, sample_n=sample_n)\n",
        "        else:\n",
        "            X, y = loader(path)\n",
        "            if sample_n is not None and len(X) > sample_n:\n",
        "                ridx = np.random.RandomState(42).choice(len(X), size=sample_n, replace=False)\n",
        "                X = X[ridx]\n",
        "                y = y[ridx]\n",
        "        print(f\"{name}: X={X.shape}, y={y.shape}\")\n",
        "\n",
        "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y if not is_multiclass else None)\n",
        "        models = make_models(use_gpu=use_gpu)\n",
        "        for mname, m in models:\n",
        "            print(f\"Training {mname} on {name} ...\")\n",
        "            stats = train_eval_one(mname, m, X_tr, y_tr, X_te, y_te, is_multiclass=is_multiclass, use_gpu=use_gpu)\n",
        "            stats[\"dataset\"] = name\n",
        "            rows.append(stats)\n",
        "            print(stats)\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Saved results to {output_csv}\")\n",
        "    return df\n",
        "\n",
        "def download_file(url, dest, chunk_size=2**20):\n",
        "    resp = requests.get(url, stream=True)\n",
        "    resp.raise_for_status()\n",
        "    with open(dest, 'wb') as f:\n",
        "        for chunk in resp.iter_content(chunk_size):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_all_datasets(data_dir='data/'):\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    # Credit Card Fraud (Figshare: альтернатива Kaggle)\n",
        "    creditcard_url = \"https://www.dropbox.com/s/b44o3t3ehmnx2b7/creditcard.csv?dl=1\"\n",
        "    creditcard_path = os.path.join(data_dir, \"creditcard.csv\")\n",
        "    if not os.path.exists(creditcard_path):\n",
        "        print(\"Downloading CreditCardFraud ...\")\n",
        "        download_file(creditcard_url, creditcard_path)\n",
        "\n",
        "    # HIGGS (UCI)\n",
        "    higgs_url = \"https://archive.ics.uci.edu/static/public/280/higgs.zip\"\n",
        "    higgs_zip = os.path.join(data_dir, \"higgs.zip\")\n",
        "    higgs_csv_gz = os.path.join(data_dir, \"HIGGS.csv.gz\")\n",
        "    if not os.path.exists(higgs_csv_gz):\n",
        "        print(\"Downloading HIGGS ...\")\n",
        "        download_file(higgs_url, higgs_zip)\n",
        "        with zipfile.ZipFile(higgs_zip) as zf:\n",
        "            zf.extract(\"HIGGS.csv.gz\", path=data_dir)\n",
        "        os.remove(higgs_zip)\n",
        "\n",
        "    # SUSY (UCI)\n",
        "    susy_url = \"https://archive.ics.uci.edu/static/public/279/susy.zip\"\n",
        "    susy_zip = os.path.join(data_dir, \"susy.zip\")\n",
        "    susy_csv_gz = os.path.join(data_dir, \"SUSY.csv.gz\")\n",
        "    if not os.path.exists(susy_csv_gz):\n",
        "        print(\"Downloading SUSY ...\")\n",
        "        download_file(susy_url, susy_zip)\n",
        "        with zipfile.ZipFile(susy_zip) as zf:\n",
        "            zf.extract(\"SUSY.csv.gz\", path=data_dir)\n",
        "        os.remove(susy_zip)\n",
        "\n",
        "    # KDDCup99 (10 percent) (UCI)\n",
        "    kdd_url = \"https://figshare.com/ndownloader/files/5976042\"\n",
        "    kdd_gz = os.path.join(data_dir, \"kddcup.data_10_percent.gz\")\n",
        "    kdd_csv = os.path.join(data_dir, \"kddcup.data_10_percent.csv\")\n",
        "    if not os.path.exists(kdd_csv):\n",
        "        print(\"Downloading KDDCup99 ...\")\n",
        "        download_file(kdd_url, kdd_gz)\n",
        "        import gzip\n",
        "        with gzip.open(kdd_gz, 'rb') as f_in, open(kdd_csv, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "        os.remove(kdd_gz)\n",
        "\n",
        "    # Covertype (UCI/sklearn, already CSV)\n",
        "    covertype_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\n",
        "    covertype_gz = os.path.join(data_dir, \"covtype.data.gz\")\n",
        "    covertype_csv = os.path.join(data_dir, \"covtype.csv\")\n",
        "    if not os.path.exists(covertype_csv):\n",
        "        print(\"Downloading Covertype ...\")\n",
        "        download_file(covertype_url, covertype_gz)\n",
        "        import gzip\n",
        "        with gzip.open(covertype_gz, 'rb') as f_in, open(covertype_csv, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "        os.remove(covertype_gz)\n",
        "\n",
        "    # HEPMASS\n",
        "    # hepmass_url = \"https://archive.ics.uci.edu/static/public/347/hepmass.zip\"\n",
        "    # hepmass_csv = os.path.join(data_dir, \"HEPMASS_train.csv\")\n",
        "    # if not os.path.exists(hepmass_csv):\n",
        "    #     print(\"Downloading HEPMASS ...\")\n",
        "    #     download_file(hepmass_url, hepmass_csv)\n",
        "\n",
        "    # Avazu CTR (HF mirror, 2m строк фрагмент — fastest for dev)\n",
        "    avazu_url = \"https://www.kaggle.com/api/v1/datasets/download/wuyingwen06/avazu-ctr-train\"\n",
        "    avazu_csv = os.path.join(data_dir, \"avazu-ctr-train.zip\")\n",
        "    if not os.path.exists(avazu_csv):\n",
        "        print(\"Downloading Avazu...\")\n",
        "        download_file(avazu_url, avazu_csv)\n",
        "\n",
        "    print(\"Done downloading all datasets.\")\n"
      ],
      "metadata": {
        "id": "PNdWaFuKwy0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir='data/'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "# hepmass_url = \"https://www.openml.org/data/get_csv/2419/BNG_balance-scale.csv\"  # Временная замена\n",
        "# hepmass_csv = os.path.join(data_dir, \"HEPMASS_train.csv\")\n",
        "# if not os.path.exists(hepmass_csv):\n",
        "#     print(\"Downloading HEPMASS (alternative dataset)...\")\n",
        "#     try:\n",
        "#         download_file(hepmass_url, hepmass_csv)\n",
        "#     except Exception as e:\n",
        "#         print(f\"Failed to download HEPMASS: {e}\")\n",
        "#         # Создать заглушку, чтобы не прерывать весь benchmark\n",
        "#         print(\"Creating dummy HEPMASS dataset...\")\n",
        "#         np.random.seed(42)\n",
        "#         X_dummy = np.random.randn(10000, 28).astype(np.float64)\n",
        "#         y_dummy = np.random.randint(0, 2, 10000).astype(np.int32)\n",
        "#         dummy_df = pd.DataFrame(X_dummy)\n",
        "#         dummy_df['label'] = y_dummy\n",
        "#         dummy_df.to_csv(hepmass_csv, index=False)\n",
        "hepmass_url = \"https://archive.ics.uci.edu/static/public/347/hepmass.zip\"\n",
        "hepmass_csv = os.path.join(data_dir, \"HEPMASS_train.csv\")\n",
        "if not os.path.exists(hepmass_csv):\n",
        "    print(\"Downloading HEPMASS ...\")\n",
        "    download_file(hepmass_url, hepmass_csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMZOBvzvqVAz",
        "outputId": "58a91e00-481f-4af8-cac4-8e4f55ef87ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading HEPMASS ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main body"
      ],
      "metadata": {
        "id": "BY4D39VIxEHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download_all_datasets(\"data/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AZBu4UVyqId",
        "outputId": "6713a741-f7d2-4c65-b1f8-d598cb6723d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading CreditCardFraud ...\n",
            "Downloading HIGGS ...\n",
            "Downloading SUSY ...\n",
            "Downloading KDDCup99 ...\n",
            "Downloading Covertype ...\n",
            "Downloading Avazu...\n",
            "Done downloading all datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qr4XnBnzzk0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example configuration; update paths to local files\n",
        "datasets = [\n",
        "    {\"name\": \"CreditCardFraud\", \"loader\": load_creditcard_fraud, \"path\": \"data/creditcard.csv\", \"multiclass\": False},\n",
        "    {\"name\": \"HIGGS\", \"loader\": load_higgs, \"path\": \"data/HIGGS.csv.gz\", \"multiclass\": False, \"sample_n\": 2_000_000},\n",
        "    {\"name\": \"SUSY\", \"loader\": load_susy, \"path\": \"data/SUSY.csv.gz\", \"multiclass\": False, \"sample_n\": 2_000_000},\n",
        "    {\"name\": \"KDDCup99\", \"loader\": load_kddcup99, \"path\": \"data/kddcup.data_10_percent.csv\", \"multiclass\": False},\n",
        "    {\"name\": \"Covertype\", \"loader\": load_covertype, \"path\": \"data/covtype.csv\", \"multiclass\": True},\n",
        "    {\"name\": \"Synthetic\", \"loader\": create_synthetic_hepmass, \"path\": \"\", \"multiclass\": False, \"sample_n\": 1_000_000},\n",
        "    {\"name\": \"Avazu\", \"loader\": load_avazu, \"path\": \"data/avazu-ctr-train.zip\", \"multiclass\": False, \"sample_n\": 2_000_000},\n",
        "]\n",
        "use_gpu = GPU_OK\n",
        "run_benchmark(datasets, use_gpu=use_gpu, output_csv=\"benchmark_results_default_impr_method.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wdW0_KHowy3a",
        "outputId": "50ec935c-83dc-4943-d0dc-1d4a8b3febe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CreditCardFraud ...\n",
            "CreditCardFraud: X=(284807, 30), y=(284807,)\n",
            "Training AdaptiveBayes on CreditCardFraud ...\n",
            "{'model': 'AdaptiveBayes', 'fit_s': 0.05635435099975439, 'pred_s': 0.04977769300012369, 'proba_s': 0.06038043499984269, 'cpu_peak_mb_fit': 0.03125, 'gpu_mem_mb_fit': 62.0, 'acc': 0.3375408166848074, 'auc': np.float64(0.9201881969726551), 'dataset': 'CreditCardFraud'}\n",
            "Training XGBoost on CreditCardFraud ...\n",
            "{'model': 'XGBoost', 'fit_s': 0.9806480430002011, 'pred_s': 0.061298986000110744, 'proba_s': 0.0693144349997965, 'cpu_peak_mb_fit': 61.74609375, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9995435553526912, 'auc': np.float64(0.9702255937546657), 'dataset': 'CreditCardFraud'}\n",
            "Training RandomForest on CreditCardFraud ...\n",
            "{'model': 'RandomForest', 'fit_s': 112.21955732800006, 'pred_s': 0.23611885399986932, 'proba_s': 0.2286607930000173, 'cpu_peak_mb_fit': 245.55078125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9996137776061234, 'auc': np.float64(0.951532496439769), 'dataset': 'CreditCardFraud'}\n",
            "Training MLP on CreditCardFraud ...\n",
            "{'model': 'MLP', 'fit_s': 34.072896668999874, 'pred_s': 0.20468860800019684, 'proba_s': 0.23855802099978973, 'cpu_peak_mb_fit': 123.15625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9982795547909132, 'auc': np.float64(0.5045023285059663), 'dataset': 'CreditCardFraud'}\n",
            "Training LightGBM on CreditCardFraud ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LightGBM', 'fit_s': 8.181892825999967, 'pred_s': 0.3385448619997078, 'proba_s': 0.3571057760000258, 'cpu_peak_mb_fit': 226.2421875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9996137776061234, 'auc': np.float64(0.9643095807540798), 'dataset': 'CreditCardFraud'}\n",
            "Training LogisticRegression on CreditCardFraud ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LogisticRegression', 'fit_s': 35.42261782800006, 'pred_s': 0.11521076700000776, 'proba_s': 0.1163425009999628, 'cpu_peak_mb_fit': 0.30078125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9982795547909132, 'auc': np.float64(0.6287717992374214), 'dataset': 'CreditCardFraud'}\n",
            "Loading HIGGS ...\n",
            "HIGGS: X=(2000000, 28), y=(2000000,)\n",
            "Training AdaptiveBayes on HIGGS ...\n",
            "{'model': 'AdaptiveBayes', 'fit_s': 0.4678551649999463, 'pred_s': 0.3237371809996148, 'proba_s': 0.2665048519997981, 'cpu_peak_mb_fit': 512.15234375, 'gpu_mem_mb_fit': 382.0, 'acc': 0.5251875, 'auc': np.float64(0.5182948206159017), 'dataset': 'HIGGS'}\n",
            "Training XGBoost on HIGGS ...\n",
            "{'model': 'XGBoost', 'fit_s': 5.150578544999917, 'pred_s': 0.2686151630000495, 'proba_s': 0.2703554750000876, 'cpu_peak_mb_fit': 12.69140625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.75044, 'auc': np.float64(0.8333018787578815), 'dataset': 'HIGGS'}\n",
            "Training RandomForest on HIGGS ...\n",
            "{'model': 'RandomForest', 'fit_s': 723.3384623279999, 'pred_s': 9.797656561000167, 'proba_s': 9.624546727000052, 'cpu_peak_mb_fit': 9682.5390625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.7439075, 'auc': np.float64(0.825254116496063), 'dataset': 'HIGGS'}\n",
            "Training MLP on HIGGS ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'MLP', 'fit_s': 492.20039164599984, 'pred_s': 3.529795269000715, 'proba_s': 3.580806881999706, 'cpu_peak_mb_fit': 929.66796875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.7568575, 'auc': np.float64(0.8392266119082334), 'dataset': 'HIGGS'}\n",
            "Training LightGBM on HIGGS ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LightGBM', 'fit_s': 32.27151433899962, 'pred_s': 5.746609759999956, 'proba_s': 5.597645398000168, 'cpu_peak_mb_fit': 76.81640625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.7386025, 'auc': np.float64(0.8198107249192417), 'dataset': 'HIGGS'}\n",
            "Training LogisticRegression on HIGGS ...\n",
            "{'model': 'LogisticRegression', 'fit_s': 17.468885637000312, 'pred_s': 0.331150534999324, 'proba_s': 0.357544923000205, 'cpu_peak_mb_fit': 0.1015625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.64197, 'auc': np.float64(0.6850412539218178), 'dataset': 'HIGGS'}\n",
            "Loading SUSY ...\n",
            "SUSY: X=(2000000, 18), y=(2000000,)\n",
            "Training AdaptiveBayes on SUSY ...\n",
            "{'model': 'AdaptiveBayes', 'fit_s': 0.36277801699998236, 'pred_s': 0.24852813000052265, 'proba_s': 0.2522159489999467, 'cpu_peak_mb_fit': 255.97265625, 'gpu_mem_mb_fit': 2.0, 'acc': 0.61073, 'auc': np.float64(0.6487821523516125), 'dataset': 'SUSY'}\n",
            "Training XGBoost on SUSY ...\n",
            "{'model': 'XGBoost', 'fit_s': 3.5692136050001864, 'pred_s': 0.26523584900041897, 'proba_s': 0.26540113000010024, 'cpu_peak_mb_fit': 8.09375, 'gpu_mem_mb_fit': 0.0, 'acc': 0.803625, 'auc': np.float64(0.876170485370527), 'dataset': 'SUSY'}\n",
            "Training RandomForest on SUSY ...\n",
            "{'model': 'RandomForest', 'fit_s': 849.5000665700009, 'pred_s': 8.828430475999994, 'proba_s': 8.977615638000316, 'cpu_peak_mb_fit': 5490.54296875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.8013525, 'auc': np.float64(0.8717799384930827), 'dataset': 'SUSY'}\n",
            "Training MLP on SUSY ...\n",
            "{'model': 'MLP', 'fit_s': 378.4551933949997, 'pred_s': 2.8349359320000076, 'proba_s': 2.669410679000066, 'cpu_peak_mb_fit': 666.67578125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.8032725, 'auc': np.float64(0.8767305105543572), 'dataset': 'SUSY'}\n",
            "Training LightGBM on SUSY ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LightGBM', 'fit_s': 28.218637543999648, 'pred_s': 5.588264336000066, 'proba_s': 5.5705148029992415, 'cpu_peak_mb_fit': 76.6640625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.80267, 'auc': np.float64(0.8752886454242074), 'dataset': 'SUSY'}\n",
            "Training LogisticRegression on SUSY ...\n",
            "{'model': 'LogisticRegression', 'fit_s': 40.2706511209999, 'pred_s': 0.30824020099953486, 'proba_s': 0.37714368799970543, 'cpu_peak_mb_fit': 0.0703125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.7887375, 'auc': np.float64(0.8580859027864128), 'dataset': 'SUSY'}\n",
            "Loading KDDCup99 ...\n",
            "KDDCup99: X=(494021, 37), y=(494021,)\n",
            "Training AdaptiveBayes on KDDCup99 ...\n",
            "{'model': 'AdaptiveBayes', 'fit_s': 0.20811140699970565, 'pred_s': 0.22783889299989823, 'proba_s': 0.21371238900064782, 'cpu_peak_mb_fit': 3.97265625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.7588279945346895, 'auc': np.float64(0.8370580306070832), 'dataset': 'KDDCup99'}\n",
            "Training XGBoost on KDDCup99 ...\n",
            "{'model': 'XGBoost', 'fit_s': 1.351349642999594, 'pred_s': 0.21917557800043141, 'proba_s': 0.22116221399937785, 'cpu_peak_mb_fit': 0.484375, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9998178229846668, 'auc': np.float64(0.9999987349513493), 'dataset': 'KDDCup99'}\n",
            "Training RandomForest on KDDCup99 ...\n",
            "{'model': 'RandomForest', 'fit_s': 15.037653116000001, 'pred_s': 0.386413285999879, 'proba_s': 0.37426352799957385, 'cpu_peak_mb_fit': 26.45703125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9997773392034816, 'auc': np.float64(0.9999996835759006), 'dataset': 'KDDCup99'}\n",
            "Training MLP on KDDCup99 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'MLP', 'fit_s': 96.08444196500022, 'pred_s': 1.5815654560001349, 'proba_s': 1.6468639579998126, 'cpu_peak_mb_fit': 14.5625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9969839583017054, 'auc': np.float64(0.9985354687876211), 'dataset': 'KDDCup99'}\n",
            "Training LightGBM on KDDCup99 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LightGBM', 'fit_s': 5.278575725000337, 'pred_s': 0.5715220359998057, 'proba_s': 0.5560120449999886, 'cpu_peak_mb_fit': 76.4296875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9995647993522595, 'auc': np.float64(0.9999978494820682), 'dataset': 'KDDCup99'}\n",
            "Training LogisticRegression on KDDCup99 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LogisticRegression', 'fit_s': 76.73566490699977, 'pred_s': 0.2220165070002622, 'proba_s': 0.22484291900036624, 'cpu_peak_mb_fit': 0.0703125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.8890845604979505, 'auc': np.float64(0.8603640413337217), 'dataset': 'KDDCup99'}\n",
            "Loading Covertype ...\n",
            "Covertype: X=(581011, 54), y=(581011,)\n",
            "Training AdaptiveBayes on Covertype ...\n",
            "{'model': 'AdaptiveBayes', 'fit_s': 0.24771340600000258, 'pred_s': 0.26046444999974483, 'proba_s': None, 'cpu_peak_mb_fit': 0.0, 'gpu_mem_mb_fit': 0.0, 'acc': 0.39666790014027176, 'auc': None, 'dataset': 'Covertype'}\n",
            "Training XGBoost on Covertype ...\n",
            "{'model': 'XGBoost', 'fit_s': 11.327623303999644, 'pred_s': 0.3308461950000492, 'proba_s': None, 'cpu_peak_mb_fit': 1.46484375, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9197352908272592, 'auc': None, 'dataset': 'Covertype'}\n",
            "Training RandomForest on Covertype ...\n",
            "{'model': 'RandomForest', 'fit_s': 72.16039070399984, 'pred_s': 1.8523380080005154, 'proba_s': None, 'cpu_peak_mb_fit': 1284.60546875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9567394989802329, 'auc': None, 'dataset': 'Covertype'}\n",
            "Training MLP on Covertype ...\n",
            "{'model': 'MLP', 'fit_s': 106.67973776899998, 'pred_s': 1.9582710459999362, 'proba_s': None, 'cpu_peak_mb_fit': 0.1796875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.7401013743190795, 'auc': None, 'dataset': 'Covertype'}\n",
            "Training LightGBM on Covertype ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LightGBM', 'fit_s': 82.16132544100037, 'pred_s': 10.201838325000608, 'proba_s': None, 'cpu_peak_mb_fit': 77.33203125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9350447062468267, 'auc': None, 'dataset': 'Covertype'}\n",
            "Training LogisticRegression on Covertype ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LogisticRegression', 'fit_s': 228.0298378460002, 'pred_s': 0.30337894199965376, 'proba_s': None, 'cpu_peak_mb_fit': 22.578125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.6920389318692288, 'auc': None, 'dataset': 'Covertype'}\n",
            "Loading Synthetic ...\n",
            "Creating synthetic HEPMASS-like dataset...\n",
            "Synthetic dataset: X=(500000, 28), y=(500000,), class balance=0.615\n",
            "Synthetic: X=(500000, 28), y=(500000,)\n",
            "Training AdaptiveBayes on Synthetic ...\n",
            "{'model': 'AdaptiveBayes', 'fit_s': 0.22422455600008107, 'pred_s': 0.23421662199962157, 'proba_s': 0.23084265499983303, 'cpu_peak_mb_fit': 0.0, 'gpu_mem_mb_fit': 0.0, 'acc': 0.53571, 'auc': np.float64(0.5292535033780479), 'dataset': 'Synthetic'}\n",
            "Training XGBoost on Synthetic ...\n",
            "{'model': 'XGBoost', 'fit_s': 2.4456621609997455, 'pred_s': 0.27357532999940304, 'proba_s': 0.2724409000002197, 'cpu_peak_mb_fit': 5.5390625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.68526, 'auc': np.float64(0.7385931095177556), 'dataset': 'Synthetic'}\n",
            "Training RandomForest on Synthetic ...\n",
            "{'model': 'RandomForest', 'fit_s': 249.44531010099945, 'pred_s': 1.867832742000246, 'proba_s': 1.8958772519999911, 'cpu_peak_mb_fit': 939.1796875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.68013, 'auc': np.float64(0.7275880164044531), 'dataset': 'Synthetic'}\n",
            "Training MLP on Synthetic ...\n",
            "{'model': 'MLP', 'fit_s': 74.29863033400034, 'pred_s': 2.100466900000356, 'proba_s': 2.0944238080001014, 'cpu_peak_mb_fit': 14.44140625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.68963, 'auc': np.float64(0.7444281658870684), 'dataset': 'Synthetic'}\n",
            "Training LightGBM on Synthetic ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LightGBM', 'fit_s': 16.41590242700022, 'pred_s': 1.2777580570000282, 'proba_s': 1.2860616030002348, 'cpu_peak_mb_fit': 76.4453125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.67989, 'auc': np.float64(0.731728526785842), 'dataset': 'Synthetic'}\n",
            "Training LogisticRegression on Synthetic ...\n",
            "{'model': 'LogisticRegression', 'fit_s': 4.193770770999436, 'pred_s': 0.3169816930003435, 'proba_s': 0.2677114719999736, 'cpu_peak_mb_fit': 0.08984375, 'gpu_mem_mb_fit': 0.0, 'acc': 0.67195, 'auc': np.float64(0.7195245912402302), 'dataset': 'Synthetic'}\n",
            "Loading Avazu ...\n",
            "Avazu: X=(1000000, 23), y=(1000000,)\n",
            "Training AdaptiveBayes on Avazu ...\n",
            "{'model': 'AdaptiveBayes', 'fit_s': 0.2670999310003026, 'pred_s': 0.267579477000254, 'proba_s': 0.26604891099941597, 'cpu_peak_mb_fit': 6.98828125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.29421, 'auc': np.float64(0.6061265301905324), 'dataset': 'Avazu'}\n",
            "Training XGBoost on Avazu ...\n",
            "{'model': 'XGBoost', 'fit_s': 2.400895048000166, 'pred_s': 0.27796304799994687, 'proba_s': 0.2859622109999691, 'cpu_peak_mb_fit': 1.86328125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.8443, 'auc': np.float64(0.7656538853155123), 'dataset': 'Avazu'}\n",
            "Training RandomForest on Avazu ...\n",
            "{'model': 'RandomForest', 'fit_s': 137.14339785299944, 'pred_s': 4.21148000500034, 'proba_s': 4.2460855829995126, 'cpu_peak_mb_fit': 3056.82421875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.83491, 'auc': np.float64(0.7487904648468557), 'dataset': 'Avazu'}\n",
            "Training MLP on Avazu ...\n",
            "{'model': 'MLP', 'fit_s': 107.26558130499961, 'pred_s': 3.678725708999991, 'proba_s': 3.299638684000456, 'cpu_peak_mb_fit': 17.07421875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.83978, 'auc': np.float64(0.5), 'dataset': 'Avazu'}\n",
            "Training LightGBM on Avazu ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LightGBM', 'fit_s': 22.02342617999966, 'pred_s': 3.332799677999901, 'proba_s': 3.3536499669999102, 'cpu_peak_mb_fit': 76.40234375, 'gpu_mem_mb_fit': 0.0, 'acc': 0.84408, 'auc': np.float64(0.7657825245402008), 'dataset': 'Avazu'}\n",
            "Training LogisticRegression on Avazu ...\n",
            "{'model': 'LogisticRegression', 'fit_s': 8.100331917999938, 'pred_s': 0.32583330899979046, 'proba_s': 0.3304923659998167, 'cpu_peak_mb_fit': 0.078125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.83978, 'auc': np.float64(0.5020758517711775), 'dataset': 'Avazu'}\n",
            "Saved results to benchmark_results_default_impr_method.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 model       fit_s     pred_s   proba_s  cpu_peak_mb_fit  \\\n",
              "0        AdaptiveBayes    0.056354   0.049778  0.060380         0.031250   \n",
              "1              XGBoost    0.980648   0.061299  0.069314        61.746094   \n",
              "2         RandomForest  112.219557   0.236119  0.228661       245.550781   \n",
              "3                  MLP   34.072897   0.204689  0.238558       123.156250   \n",
              "4             LightGBM    8.181893   0.338545  0.357106       226.242188   \n",
              "5   LogisticRegression   35.422618   0.115211  0.116343         0.300781   \n",
              "6        AdaptiveBayes    0.467855   0.323737  0.266505       512.152344   \n",
              "7              XGBoost    5.150579   0.268615  0.270355        12.691406   \n",
              "8         RandomForest  723.338462   9.797657  9.624547      9682.539062   \n",
              "9                  MLP  492.200392   3.529795  3.580807       929.667969   \n",
              "10            LightGBM   32.271514   5.746610  5.597645        76.816406   \n",
              "11  LogisticRegression   17.468886   0.331151  0.357545         0.101562   \n",
              "12       AdaptiveBayes    0.362778   0.248528  0.252216       255.972656   \n",
              "13             XGBoost    3.569214   0.265236  0.265401         8.093750   \n",
              "14        RandomForest  849.500067   8.828430  8.977616      5490.542969   \n",
              "15                 MLP  378.455193   2.834936  2.669411       666.675781   \n",
              "16            LightGBM   28.218638   5.588264  5.570515        76.664062   \n",
              "17  LogisticRegression   40.270651   0.308240  0.377144         0.070312   \n",
              "18       AdaptiveBayes    0.208111   0.227839  0.213712         3.972656   \n",
              "19             XGBoost    1.351350   0.219176  0.221162         0.484375   \n",
              "20        RandomForest   15.037653   0.386413  0.374264        26.457031   \n",
              "21                 MLP   96.084442   1.581565  1.646864        14.562500   \n",
              "22            LightGBM    5.278576   0.571522  0.556012        76.429688   \n",
              "23  LogisticRegression   76.735665   0.222017  0.224843         0.070312   \n",
              "24       AdaptiveBayes    0.247713   0.260464       NaN         0.000000   \n",
              "25             XGBoost   11.327623   0.330846       NaN         1.464844   \n",
              "26        RandomForest   72.160391   1.852338       NaN      1284.605469   \n",
              "27                 MLP  106.679738   1.958271       NaN         0.179688   \n",
              "28            LightGBM   82.161325  10.201838       NaN        77.332031   \n",
              "29  LogisticRegression  228.029838   0.303379       NaN        22.578125   \n",
              "30       AdaptiveBayes    0.224225   0.234217  0.230843         0.000000   \n",
              "31             XGBoost    2.445662   0.273575  0.272441         5.539062   \n",
              "32        RandomForest  249.445310   1.867833  1.895877       939.179688   \n",
              "33                 MLP   74.298630   2.100467  2.094424        14.441406   \n",
              "34            LightGBM   16.415902   1.277758  1.286062        76.445312   \n",
              "35  LogisticRegression    4.193771   0.316982  0.267711         0.089844   \n",
              "36       AdaptiveBayes    0.267100   0.267579  0.266049         6.988281   \n",
              "37             XGBoost    2.400895   0.277963  0.285962         1.863281   \n",
              "38        RandomForest  137.143398   4.211480  4.246086      3056.824219   \n",
              "39                 MLP  107.265581   3.678726  3.299639        17.074219   \n",
              "40            LightGBM   22.023426   3.332800  3.353650        76.402344   \n",
              "41  LogisticRegression    8.100332   0.325833  0.330492         0.078125   \n",
              "\n",
              "    gpu_mem_mb_fit       acc       auc          dataset  \n",
              "0             62.0  0.337541  0.920188  CreditCardFraud  \n",
              "1              0.0  0.999544  0.970226  CreditCardFraud  \n",
              "2              0.0  0.999614  0.951532  CreditCardFraud  \n",
              "3              0.0  0.998280  0.504502  CreditCardFraud  \n",
              "4              0.0  0.999614  0.964310  CreditCardFraud  \n",
              "5              0.0  0.998280  0.628772  CreditCardFraud  \n",
              "6            382.0  0.525188  0.518295            HIGGS  \n",
              "7              0.0  0.750440  0.833302            HIGGS  \n",
              "8              0.0  0.743908  0.825254            HIGGS  \n",
              "9              0.0  0.756857  0.839227            HIGGS  \n",
              "10             0.0  0.738603  0.819811            HIGGS  \n",
              "11             0.0  0.641970  0.685041            HIGGS  \n",
              "12             2.0  0.610730  0.648782             SUSY  \n",
              "13             0.0  0.803625  0.876170             SUSY  \n",
              "14             0.0  0.801353  0.871780             SUSY  \n",
              "15             0.0  0.803272  0.876731             SUSY  \n",
              "16             0.0  0.802670  0.875289             SUSY  \n",
              "17             0.0  0.788737  0.858086             SUSY  \n",
              "18             0.0  0.758828  0.837058         KDDCup99  \n",
              "19             0.0  0.999818  0.999999         KDDCup99  \n",
              "20             0.0  0.999777  1.000000         KDDCup99  \n",
              "21             0.0  0.996984  0.998535         KDDCup99  \n",
              "22             0.0  0.999565  0.999998         KDDCup99  \n",
              "23             0.0  0.889085  0.860364         KDDCup99  \n",
              "24             0.0  0.396668       NaN        Covertype  \n",
              "25             0.0  0.919735       NaN        Covertype  \n",
              "26             0.0  0.956739       NaN        Covertype  \n",
              "27             0.0  0.740101       NaN        Covertype  \n",
              "28             0.0  0.935045       NaN        Covertype  \n",
              "29             0.0  0.692039       NaN        Covertype  \n",
              "30             0.0  0.535710  0.529254        Synthetic  \n",
              "31             0.0  0.685260  0.738593        Synthetic  \n",
              "32             0.0  0.680130  0.727588        Synthetic  \n",
              "33             0.0  0.689630  0.744428        Synthetic  \n",
              "34             0.0  0.679890  0.731729        Synthetic  \n",
              "35             0.0  0.671950  0.719525        Synthetic  \n",
              "36             0.0  0.294210  0.606127            Avazu  \n",
              "37             0.0  0.844300  0.765654            Avazu  \n",
              "38             0.0  0.834910  0.748790            Avazu  \n",
              "39             0.0  0.839780  0.500000            Avazu  \n",
              "40             0.0  0.844080  0.765783            Avazu  \n",
              "41             0.0  0.839780  0.502076            Avazu  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6647ffd9-6f2c-4446-ac69-a4de61e8c470\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>fit_s</th>\n",
              "      <th>pred_s</th>\n",
              "      <th>proba_s</th>\n",
              "      <th>cpu_peak_mb_fit</th>\n",
              "      <th>gpu_mem_mb_fit</th>\n",
              "      <th>acc</th>\n",
              "      <th>auc</th>\n",
              "      <th>dataset</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaptiveBayes</td>\n",
              "      <td>0.056354</td>\n",
              "      <td>0.049778</td>\n",
              "      <td>0.060380</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0.337541</td>\n",
              "      <td>0.920188</td>\n",
              "      <td>CreditCardFraud</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.980648</td>\n",
              "      <td>0.061299</td>\n",
              "      <td>0.069314</td>\n",
              "      <td>61.746094</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.999544</td>\n",
              "      <td>0.970226</td>\n",
              "      <td>CreditCardFraud</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>112.219557</td>\n",
              "      <td>0.236119</td>\n",
              "      <td>0.228661</td>\n",
              "      <td>245.550781</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.999614</td>\n",
              "      <td>0.951532</td>\n",
              "      <td>CreditCardFraud</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MLP</td>\n",
              "      <td>34.072897</td>\n",
              "      <td>0.204689</td>\n",
              "      <td>0.238558</td>\n",
              "      <td>123.156250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.998280</td>\n",
              "      <td>0.504502</td>\n",
              "      <td>CreditCardFraud</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>8.181893</td>\n",
              "      <td>0.338545</td>\n",
              "      <td>0.357106</td>\n",
              "      <td>226.242188</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.999614</td>\n",
              "      <td>0.964310</td>\n",
              "      <td>CreditCardFraud</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>35.422618</td>\n",
              "      <td>0.115211</td>\n",
              "      <td>0.116343</td>\n",
              "      <td>0.300781</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.998280</td>\n",
              "      <td>0.628772</td>\n",
              "      <td>CreditCardFraud</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>AdaptiveBayes</td>\n",
              "      <td>0.467855</td>\n",
              "      <td>0.323737</td>\n",
              "      <td>0.266505</td>\n",
              "      <td>512.152344</td>\n",
              "      <td>382.0</td>\n",
              "      <td>0.525188</td>\n",
              "      <td>0.518295</td>\n",
              "      <td>HIGGS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>5.150579</td>\n",
              "      <td>0.268615</td>\n",
              "      <td>0.270355</td>\n",
              "      <td>12.691406</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.750440</td>\n",
              "      <td>0.833302</td>\n",
              "      <td>HIGGS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>723.338462</td>\n",
              "      <td>9.797657</td>\n",
              "      <td>9.624547</td>\n",
              "      <td>9682.539062</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.743908</td>\n",
              "      <td>0.825254</td>\n",
              "      <td>HIGGS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>MLP</td>\n",
              "      <td>492.200392</td>\n",
              "      <td>3.529795</td>\n",
              "      <td>3.580807</td>\n",
              "      <td>929.667969</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.756857</td>\n",
              "      <td>0.839227</td>\n",
              "      <td>HIGGS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>32.271514</td>\n",
              "      <td>5.746610</td>\n",
              "      <td>5.597645</td>\n",
              "      <td>76.816406</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.738603</td>\n",
              "      <td>0.819811</td>\n",
              "      <td>HIGGS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>17.468886</td>\n",
              "      <td>0.331151</td>\n",
              "      <td>0.357545</td>\n",
              "      <td>0.101562</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.641970</td>\n",
              "      <td>0.685041</td>\n",
              "      <td>HIGGS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>AdaptiveBayes</td>\n",
              "      <td>0.362778</td>\n",
              "      <td>0.248528</td>\n",
              "      <td>0.252216</td>\n",
              "      <td>255.972656</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.610730</td>\n",
              "      <td>0.648782</td>\n",
              "      <td>SUSY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>3.569214</td>\n",
              "      <td>0.265236</td>\n",
              "      <td>0.265401</td>\n",
              "      <td>8.093750</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.803625</td>\n",
              "      <td>0.876170</td>\n",
              "      <td>SUSY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>849.500067</td>\n",
              "      <td>8.828430</td>\n",
              "      <td>8.977616</td>\n",
              "      <td>5490.542969</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.801353</td>\n",
              "      <td>0.871780</td>\n",
              "      <td>SUSY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>MLP</td>\n",
              "      <td>378.455193</td>\n",
              "      <td>2.834936</td>\n",
              "      <td>2.669411</td>\n",
              "      <td>666.675781</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.803272</td>\n",
              "      <td>0.876731</td>\n",
              "      <td>SUSY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>28.218638</td>\n",
              "      <td>5.588264</td>\n",
              "      <td>5.570515</td>\n",
              "      <td>76.664062</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.802670</td>\n",
              "      <td>0.875289</td>\n",
              "      <td>SUSY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>40.270651</td>\n",
              "      <td>0.308240</td>\n",
              "      <td>0.377144</td>\n",
              "      <td>0.070312</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.788737</td>\n",
              "      <td>0.858086</td>\n",
              "      <td>SUSY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>AdaptiveBayes</td>\n",
              "      <td>0.208111</td>\n",
              "      <td>0.227839</td>\n",
              "      <td>0.213712</td>\n",
              "      <td>3.972656</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.758828</td>\n",
              "      <td>0.837058</td>\n",
              "      <td>KDDCup99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>1.351350</td>\n",
              "      <td>0.219176</td>\n",
              "      <td>0.221162</td>\n",
              "      <td>0.484375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.999818</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>KDDCup99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>15.037653</td>\n",
              "      <td>0.386413</td>\n",
              "      <td>0.374264</td>\n",
              "      <td>26.457031</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.999777</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>KDDCup99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>MLP</td>\n",
              "      <td>96.084442</td>\n",
              "      <td>1.581565</td>\n",
              "      <td>1.646864</td>\n",
              "      <td>14.562500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.996984</td>\n",
              "      <td>0.998535</td>\n",
              "      <td>KDDCup99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>5.278576</td>\n",
              "      <td>0.571522</td>\n",
              "      <td>0.556012</td>\n",
              "      <td>76.429688</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.999565</td>\n",
              "      <td>0.999998</td>\n",
              "      <td>KDDCup99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>76.735665</td>\n",
              "      <td>0.222017</td>\n",
              "      <td>0.224843</td>\n",
              "      <td>0.070312</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.889085</td>\n",
              "      <td>0.860364</td>\n",
              "      <td>KDDCup99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>AdaptiveBayes</td>\n",
              "      <td>0.247713</td>\n",
              "      <td>0.260464</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.396668</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Covertype</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>11.327623</td>\n",
              "      <td>0.330846</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.464844</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.919735</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Covertype</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>72.160391</td>\n",
              "      <td>1.852338</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1284.605469</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.956739</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Covertype</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>MLP</td>\n",
              "      <td>106.679738</td>\n",
              "      <td>1.958271</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.179688</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.740101</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Covertype</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>82.161325</td>\n",
              "      <td>10.201838</td>\n",
              "      <td>NaN</td>\n",
              "      <td>77.332031</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.935045</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Covertype</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>228.029838</td>\n",
              "      <td>0.303379</td>\n",
              "      <td>NaN</td>\n",
              "      <td>22.578125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.692039</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Covertype</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>AdaptiveBayes</td>\n",
              "      <td>0.224225</td>\n",
              "      <td>0.234217</td>\n",
              "      <td>0.230843</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.535710</td>\n",
              "      <td>0.529254</td>\n",
              "      <td>Synthetic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>2.445662</td>\n",
              "      <td>0.273575</td>\n",
              "      <td>0.272441</td>\n",
              "      <td>5.539062</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.685260</td>\n",
              "      <td>0.738593</td>\n",
              "      <td>Synthetic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>249.445310</td>\n",
              "      <td>1.867833</td>\n",
              "      <td>1.895877</td>\n",
              "      <td>939.179688</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.680130</td>\n",
              "      <td>0.727588</td>\n",
              "      <td>Synthetic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>MLP</td>\n",
              "      <td>74.298630</td>\n",
              "      <td>2.100467</td>\n",
              "      <td>2.094424</td>\n",
              "      <td>14.441406</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.689630</td>\n",
              "      <td>0.744428</td>\n",
              "      <td>Synthetic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>16.415902</td>\n",
              "      <td>1.277758</td>\n",
              "      <td>1.286062</td>\n",
              "      <td>76.445312</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.679890</td>\n",
              "      <td>0.731729</td>\n",
              "      <td>Synthetic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>4.193771</td>\n",
              "      <td>0.316982</td>\n",
              "      <td>0.267711</td>\n",
              "      <td>0.089844</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.671950</td>\n",
              "      <td>0.719525</td>\n",
              "      <td>Synthetic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>AdaptiveBayes</td>\n",
              "      <td>0.267100</td>\n",
              "      <td>0.267579</td>\n",
              "      <td>0.266049</td>\n",
              "      <td>6.988281</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.294210</td>\n",
              "      <td>0.606127</td>\n",
              "      <td>Avazu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>2.400895</td>\n",
              "      <td>0.277963</td>\n",
              "      <td>0.285962</td>\n",
              "      <td>1.863281</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.844300</td>\n",
              "      <td>0.765654</td>\n",
              "      <td>Avazu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>137.143398</td>\n",
              "      <td>4.211480</td>\n",
              "      <td>4.246086</td>\n",
              "      <td>3056.824219</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.834910</td>\n",
              "      <td>0.748790</td>\n",
              "      <td>Avazu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>MLP</td>\n",
              "      <td>107.265581</td>\n",
              "      <td>3.678726</td>\n",
              "      <td>3.299639</td>\n",
              "      <td>17.074219</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.839780</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>Avazu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>22.023426</td>\n",
              "      <td>3.332800</td>\n",
              "      <td>3.353650</td>\n",
              "      <td>76.402344</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.844080</td>\n",
              "      <td>0.765783</td>\n",
              "      <td>Avazu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>8.100332</td>\n",
              "      <td>0.325833</td>\n",
              "      <td>0.330492</td>\n",
              "      <td>0.078125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.839780</td>\n",
              "      <td>0.502076</td>\n",
              "      <td>Avazu</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6647ffd9-6f2c-4446-ac69-a4de61e8c470')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6647ffd9-6f2c-4446-ac69-a4de61e8c470 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6647ffd9-6f2c-4446-ac69-a4de61e8c470');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-deae7128-9448-4771-bc7a-acd7100a8436\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-deae7128-9448-4771-bc7a-acd7100a8436')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-deae7128-9448-4771-bc7a-acd7100a8436 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"run_benchmark(datasets, use_gpu=use_gpu, output_csv=\\\"benchmark_results_default_impr_method\",\n  \"rows\": 42,\n  \"fields\": [\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"AdaptiveBayes\",\n          \"XGBoost\",\n          \"LogisticRegression\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fit_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 187.55859095773363,\n        \"min\": 0.05635435099975439,\n        \"max\": 849.5000665700009,\n        \"num_unique_values\": 42,\n        \"samples\": [\n          11.327623303999644,\n          3.5692136050001864,\n          723.3384623279999\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.66575056177276,\n        \"min\": 0.04977769300012369,\n        \"max\": 10.201838325000608,\n        \"num_unique_values\": 42,\n        \"samples\": [\n          0.3308461950000492,\n          0.26523584900041897,\n          9.797656561000167\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"proba_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.4500393332667247,\n        \"min\": 0.06038043499984269,\n        \"max\": 9.624546727000052,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          0.3304923659998167,\n          0.26540113000010024,\n          1.8958772519999911\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cpu_peak_mb_fit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1737.5468911417952,\n        \"min\": 0.0,\n        \"max\": 9682.5390625,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          0.484375,\n          76.6640625,\n          666.67578125\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gpu_mem_mb_fit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 59.47655917883757,\n        \"min\": 0.0,\n        \"max\": 382.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          2.0,\n          62.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18166236939844216,\n        \"min\": 0.29421,\n        \"max\": 0.9998178229846668,\n        \"num_unique_values\": 39,\n        \"samples\": [\n          0.67195,\n          0.83491,\n          0.5251875\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"auc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1538730225064065,\n        \"min\": 0.5,\n        \"max\": 0.9999996835759006,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          0.5020758517711775,\n          0.876170485370527,\n          0.7275880164044531\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dataset\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"CreditCardFraud\",\n          \"HIGGS\",\n          \"Synthetic\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optuna optimizing"
      ],
      "metadata": {
        "id": "xdTHFSU8AXYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "import json"
      ],
      "metadata": {
        "id": "jA1_p1LTAxKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def optimize_adaptive_bayes_params(X, y,\n",
        "          dataset_name,\n",
        "          n_trials=50,\n",
        "          cv_folds=3,\n",
        "          use_gpu=False,\n",
        "          verbose=False,\n",
        "          timeout=3600):\n",
        "    \"\"\"\n",
        "    Оптимизация гиперпараметров AdaptiveBayes для конкретного датасета\n",
        "\n",
        "    Args:\n",
        "        X: признаки\n",
        "        y: целевая переменная\n",
        "        dataset_name: название датасета\n",
        "        n_trials: количество проб Optuna\n",
        "        cv_folds: количество фолдов кросс-валидации\n",
        "        use_gpu: использовать GPU\n",
        "        timeout: максимальное время оптимизации в секундах\n",
        "\n",
        "    Returns:\n",
        "        dict: лучшие параметры\n",
        "    \"\"\"\n",
        "    if not verbose:\n",
        "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "    print(f\"Optimizing AdaptiveBayes parameters for {dataset_name}...\")\n",
        "    print(f\"Dataset shape: {X.shape}, classes: {len(np.unique(y))}\")\n",
        "\n",
        "    # Нормализация данных (важно для AdaptiveBayes)\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Определить тип задачи\n",
        "    is_binary = len(np.unique(y)) == 2\n",
        "    scoring = 'roc_auc' if is_binary else 'accuracy'\n",
        "\n",
        "    # Стратегия кросс-валидации\n",
        "    if is_binary:\n",
        "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "    else:\n",
        "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    def objective(trial):\n",
        "        # Предлагаемые параметры\n",
        "        base_lr = trial.suggest_float('base_lr', 1e-4, 1.0, log=True)\n",
        "        eps = trial.suggest_float('eps', 1e-12, 1e-6, log=True)\n",
        "        epochs = trial.suggest_int('epochs', 1, 10)\n",
        "        batch_size = trial.suggest_categorical('batch_size', [1024, 2048, 4096, 8192, 16384, 32768, 65536])\n",
        "\n",
        "        try:\n",
        "            # Создать модель с предложенными параметрами\n",
        "            model = AdaptiveBayes(\n",
        "                base_lr=base_lr,\n",
        "                eps=eps,\n",
        "                device='gpu' if use_gpu else 'cpu'\n",
        "            )\n",
        "\n",
        "            # Кросс-валидация\n",
        "            scores = []\n",
        "            for train_idx, val_idx in cv.split(X_scaled, y):\n",
        "                X_train_fold = X_scaled[train_idx]\n",
        "                X_val_fold = X_scaled[val_idx]\n",
        "                y_train_fold = y[train_idx]\n",
        "                y_val_fold = y[val_idx]\n",
        "\n",
        "                # Обучение\n",
        "                model.fit(X_train_fold, y_train_fold, epochs=epochs, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "                # Предсказание\n",
        "                if is_binary:\n",
        "                    y_pred_proba = model.predict_proba(X_val_fold)\n",
        "                    score = roc_auc_score(y_val_fold, y_pred_proba)\n",
        "                else:\n",
        "                    y_pred = model.predict(X_val_fold)\n",
        "                    score = accuracy_score(y_val_fold, y_pred)\n",
        "\n",
        "                scores.append(score)\n",
        "\n",
        "            mean_score = np.mean(scores)\n",
        "\n",
        "            # Логирование промежуточных результатов\n",
        "            # print(f\"Trial {trial.number}: base_lr={base_lr:.1e}, eps={eps:.1e}, \"\n",
        "            #       f\"epochs={epochs}, batch_size={batch_size}, score={mean_score:.4f}\")\n",
        "\n",
        "            return mean_score\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Trial {trial.number} failed: {e}\")\n",
        "            return 0.0  # Минимальная оценка при ошибке\n",
        "\n",
        "    # Создать study\n",
        "    study = optuna.create_study(\n",
        "        direction='maximize',\n",
        "        study_name=f'adaptive_bayes_{dataset_name}',\n",
        "        sampler=optuna.samplers.TPESampler(seed=42)\n",
        "    )\n",
        "\n",
        "    # Оптимизация\n",
        "    study.optimize(objective, n_trials=n_trials, timeout=timeout, show_progress_bar=True)\n",
        "\n",
        "    print(f\"\\nOptimization completed for {dataset_name}:\")\n",
        "    print(f\"Best score: {study.best_value:.4f}\")\n",
        "    print(f\"Best params: {study.best_params}\")\n",
        "\n",
        "    return study.best_params\n",
        "\n",
        "def run_hyperparameter_optimization(datasets_config, use_gpu=False, n_trials=50, save_path=\"adaptive_bayes_params.json\"):\n",
        "    \"\"\"\n",
        "    Запуск оптимизации параметров для всех датасетов\n",
        "\n",
        "    Args:\n",
        "        datasets_config: конфигурация датасетов\n",
        "        use_gpu: использовать GPU\n",
        "        n_trials: количество проб для каждого датасета\n",
        "        save_path: путь для сохранения параметров\n",
        "\n",
        "    Returns:\n",
        "        dict: словарь параметров для каждого датасета\n",
        "    \"\"\"\n",
        "\n",
        "    optimized_params = {}\n",
        "\n",
        "    for ds in datasets_config:\n",
        "        name = ds[\"name\"]\n",
        "        loader = ds[\"loader\"]\n",
        "        path = ds[\"path\"]\n",
        "        sample_n = ds.get(\"sample_n\")\n",
        "\n",
        "        try:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"Loading {name} for optimization...\")\n",
        "\n",
        "            # Загрузить данные\n",
        "            if name == \"Avazu\":\n",
        "                X, y = load_avazu(path, sample_n=sample_n)\n",
        "            elif name == \"Synthetic\":\n",
        "                X, y = create_synthetic_hepmass()\n",
        "            else:\n",
        "                X, y = loader(path)\n",
        "                if sample_n is not None and len(X) > sample_n:\n",
        "                    ridx = np.random.RandomState(42).choice(len(X), size=sample_n, replace=False)\n",
        "                    X = X[ridx]\n",
        "                    y = y[ridx]\n",
        "\n",
        "            # Уменьшить размер для оптимизации (ускорение)\n",
        "            optimization_size = min(100000, len(X))\n",
        "            if len(X) > optimization_size:\n",
        "                ridx = np.random.RandomState(42).choice(len(X), size=optimization_size, replace=False)\n",
        "                X_opt = X[ridx]\n",
        "                y_opt = y[ridx]\n",
        "            else:\n",
        "                X_opt = X\n",
        "                y_opt = y\n",
        "\n",
        "            print(f\"Optimization subset: {X_opt.shape}\")\n",
        "\n",
        "            # Оптимизация параметров\n",
        "            best_params = optimize_adaptive_bayes_params(\n",
        "                X_opt, y_opt,\n",
        "                dataset_name=name,\n",
        "                n_trials=n_trials,\n",
        "                cv_folds=3,\n",
        "                use_gpu=use_gpu,\n",
        "                timeout=1800  # 30 минут на датасет\n",
        "            )\n",
        "\n",
        "            optimized_params[name] = best_params\n",
        "\n",
        "            # Промежуточное сохранение\n",
        "            with open(save_path, 'w') as f:\n",
        "                json.dump(optimized_params, f, indent=2)\n",
        "\n",
        "            print(f\"Saved intermediate results to {save_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to optimize {name}: {e}\")\n",
        "            # Использовать параметры по умолчанию\n",
        "            optimized_params[name] = {\n",
        "                'base_lr': 0.01,\n",
        "                'eps': 1e-10,\n",
        "                'epochs': 1,\n",
        "                'batch_size': 65536\n",
        "            }\n",
        "\n",
        "    # Финальное сохранение\n",
        "    with open(save_path, 'w') as f:\n",
        "        json.dump(optimized_params, f, indent=2)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Hyperparameter optimization completed!\")\n",
        "    print(f\"Results saved to: {save_path}\")\n",
        "\n",
        "    # Вывести сводку\n",
        "    print(\"\\nOptimized parameters summary:\")\n",
        "    for dataset, params in optimized_params.items():\n",
        "        print(f\"{dataset}: lr={params['base_lr']:.1e}, eps={params['eps']:.1e}, \"\n",
        "              f\"epochs={params['epochs']}, batch_size={params['batch_size']}\")\n",
        "\n",
        "    return optimized_params"
      ],
      "metadata": {
        "id": "An0GVGGqwy9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Этап 1: Оптимизация гиперпараметров\n",
        "    print(\"Starting hyperparameter optimization...\")\n",
        "    optimized_params = run_hyperparameter_optimization(\n",
        "        datasets,\n",
        "        use_gpu=use_gpu,\n",
        "        n_trials=1000,\n",
        "        save_path=\"adaptive_bayes_params.json\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0eda1f5b72ec472fbcafcd50af9c29b6",
            "727a82b7eeae4aad96eedc1cad1c2902",
            "dd74db8bbda34a72abad0c59bda97b58",
            "ee1a1ceb09784e7fad8aa5297c4e6d0d",
            "5999d83209444c2994430ea13b32071f",
            "e5114ee1c15c4e128cd72dcc8d0db4c0",
            "d17e0c4dd34f40ab8f65e8eb8b7751ca",
            "51b2ea1a6c5b4ee293d0bbfd6f359a58",
            "f0b3d4764f3c4abcac81b16e7b60eb6c",
            "1abb012ac31e4fb989870cf2af69d87e",
            "e9e9911dca7043268cb41d976be01e1b",
            "91dcad525bab496ab8aacc32bab897b6",
            "cb519d3358d14ecfa05a8d755408db9a",
            "52586f3269a7454fb0cf360416cdd742",
            "f513141b5e9c4d049ea59fd56987c4e0",
            "6ce0902c2ad749f3a55e086fb5395ae9",
            "603c975075994e42a933a241910870d5",
            "2ecf146da98c40dd86bade74c049bce7",
            "b7c2e0573be34062b0a307b46dc8968f",
            "7a84b10cb90a472fa1f046887b8af884",
            "9e049e9470934e8396014caf56fd80b0",
            "062e968cb3cf4c7dbe80e257f4a16cc6",
            "72bebdc79b884c32bd4423849177993b",
            "b9e08d76548f4a0185a192aa56d5a0a3",
            "d56b2f72fe2c45d991460c97cc737915",
            "2831a967073e4c4381308363b112a973",
            "2965355fddfb48ae88881a4ddf3197f0",
            "5c9ba410ff804da691d997f044364003",
            "426eb09e48e2420c95dcf9a4c5099b13",
            "dfb13e75c790419fa3c6b36c936ba011",
            "bac80ec77eab4415b6bf891a39130434",
            "b106766f073a405aab81d7fc470fd2ec",
            "4fdb5e945e644af9b4d66362ea5a63b5",
            "3504d21273f04addb0b5c86b93adbb29",
            "d4f4f356d5874c3b9639c7875eeaa567"
          ]
        },
        "id": "7mm03uYiAch9",
        "outputId": "a69904b3-e6a1-43f3-e295-910f641a84ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting hyperparameter optimization...\n",
            "\n",
            "============================================================\n",
            "Loading CreditCardFraud for optimization...\n",
            "Optimization subset: (100000, 30)\n",
            "Optimizing AdaptiveBayes parameters for CreditCardFraud...\n",
            "Dataset shape: (100000, 30), classes: 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0eda1f5b72ec472fbcafcd50af9c29b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimization completed for CreditCardFraud:\n",
            "Best score: 0.9496\n",
            "Best params: {'base_lr': 0.0035801347101830825, 'eps': 5.602651860822652e-08, 'epochs': 8, 'batch_size': 8192}\n",
            "Saved intermediate results to adaptive_bayes_params.json\n",
            "\n",
            "============================================================\n",
            "Loading HIGGS for optimization...\n",
            "Optimization subset: (100000, 28)\n",
            "Optimizing AdaptiveBayes parameters for HIGGS...\n",
            "Dataset shape: (100000, 28), classes: 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "727a82b7eeae4aad96eedc1cad1c2902"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimization completed for HIGGS:\n",
            "Best score: 0.5443\n",
            "Best params: {'base_lr': 0.0055219579432839744, 'eps': 3.2468692859726945e-09, 'epochs': 8, 'batch_size': 4096}\n",
            "Saved intermediate results to adaptive_bayes_params.json\n",
            "\n",
            "============================================================\n",
            "Loading SUSY for optimization...\n",
            "Optimization subset: (100000, 18)\n",
            "Optimizing AdaptiveBayes parameters for SUSY...\n",
            "Dataset shape: (100000, 18), classes: 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd74db8bbda34a72abad0c59bda97b58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimization completed for SUSY:\n",
            "Best score: 0.7116\n",
            "Best params: {'base_lr': 0.0013706424415083963, 'eps': 1.922300840428287e-09, 'epochs': 10, 'batch_size': 1024}\n",
            "Saved intermediate results to adaptive_bayes_params.json\n",
            "\n",
            "============================================================\n",
            "Loading KDDCup99 for optimization...\n",
            "Optimization subset: (100000, 37)\n",
            "Optimizing AdaptiveBayes parameters for KDDCup99...\n",
            "Dataset shape: (100000, 37), classes: 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee1a1ceb09784e7fad8aa5297c4e6d0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimization completed for KDDCup99:\n",
            "Best score: 0.9942\n",
            "Best params: {'base_lr': 0.018859749269881067, 'eps': 9.343459559523171e-11, 'epochs': 1, 'batch_size': 8192}\n",
            "Saved intermediate results to adaptive_bayes_params.json\n",
            "\n",
            "============================================================\n",
            "Loading Covertype for optimization...\n",
            "Optimization subset: (100000, 54)\n",
            "Optimizing AdaptiveBayes parameters for Covertype...\n",
            "Dataset shape: (100000, 54), classes: 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5999d83209444c2994430ea13b32071f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimization completed for Covertype:\n",
            "Best score: 0.5056\n",
            "Best params: {'base_lr': 0.01527223129800899, 'eps': 5.1418064711309445e-12, 'epochs': 4, 'batch_size': 65536}\n",
            "Saved intermediate results to adaptive_bayes_params.json\n",
            "\n",
            "============================================================\n",
            "Loading Synthetic for optimization...\n",
            "Creating synthetic HEPMASS-like dataset...\n",
            "Synthetic dataset: X=(500000, 28), y=(500000,), class balance=0.615\n",
            "Optimization subset: (100000, 28)\n",
            "Optimizing AdaptiveBayes parameters for Synthetic...\n",
            "Dataset shape: (100000, 28), classes: 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5114ee1c15c4e128cd72dcc8d0db4c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimization completed for Synthetic:\n",
            "Best score: 0.5558\n",
            "Best params: {'base_lr': 0.19116879230883235, 'eps': 1.3887387911209016e-11, 'epochs': 3, 'batch_size': 1024}\n",
            "Saved intermediate results to adaptive_bayes_params.json\n",
            "\n",
            "============================================================\n",
            "Loading Avazu for optimization...\n",
            "Optimization subset: (100000, 23)\n",
            "Optimizing AdaptiveBayes parameters for Avazu...\n",
            "Dataset shape: (100000, 23), classes: 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d17e0c4dd34f40ab8f65e8eb8b7751ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimization completed for Avazu:\n",
            "Best score: 0.6153\n",
            "Best params: {'base_lr': 0.005782178469374114, 'eps': 4.808686204831486e-07, 'epochs': 7, 'batch_size': 2048}\n",
            "Saved intermediate results to adaptive_bayes_params.json\n",
            "\n",
            "============================================================\n",
            "Hyperparameter optimization completed!\n",
            "Results saved to: adaptive_bayes_params.json\n",
            "\n",
            "Optimized parameters summary:\n",
            "CreditCardFraud: lr=3.6e-03, eps=5.6e-08, epochs=8, batch_size=8192\n",
            "HIGGS: lr=5.5e-03, eps=3.2e-09, epochs=8, batch_size=4096\n",
            "SUSY: lr=1.4e-03, eps=1.9e-09, epochs=10, batch_size=1024\n",
            "KDDCup99: lr=1.9e-02, eps=9.3e-11, epochs=1, batch_size=8192\n",
            "Covertype: lr=1.5e-02, eps=5.1e-12, epochs=4, batch_size=65536\n",
            "Synthetic: lr=1.9e-01, eps=1.4e-11, epochs=3, batch_size=1024\n",
            "Avazu: lr=5.8e-03, eps=4.8e-07, epochs=7, batch_size=2048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Optimized parameters summary: n_trials=100\n",
        "CreditCardFraud: lr=1.4e-04, eps=1.8e-09, epochs=1, batch_size=2048\n",
        "HIGGS: lr=7.6e-04, eps=5.3e-12, epochs=10, batch_size=2048\n",
        "SUSY: lr=1.6e-04, eps=2.3e-09, epochs=5, batch_size=1024\n",
        "KDDCup99: lr=1.0e-03, eps=9.4e-07, epochs=6, batch_size=8192\n",
        "Covertype: lr=3.2e-03, eps=3.0e-10, epochs=1, batch_size=65536\n",
        "Synthetic: lr=1.0e-04, eps=2.3e-09, epochs=3, batch_size=8192\n",
        "Avazu: lr=1.3e-04, eps=2.3e-07, epochs=1, batch_size=2048\n",
        "\n",
        "Optimized parameters summary: n_trials=500\n",
        "CreditCardFraud: lr=1.3e-03, eps=4.8e-10, epochs=5, batch_size=4096 Best score: 0.9373\n",
        "HIGGS: lr=9.5e-01, eps=1.1e-12, epochs=4, batch_size=1024 Best score: 0.6061\n",
        "SUSY: lr=1.1e-04, eps=6.2e-09, epochs=9, batch_size=8192 Best score: 0.8080\n",
        "KDDCup99: lr=1.2e-02, eps=3.9e-10, epochs=4, batch_size=2048 Best score: 0.9973\n",
        "Covertype: lr=5.9e-03, eps=1.1e-10, epochs=1, batch_size=32768 Best score: 0.4172\n",
        "Synthetic: lr=1.5e-04, eps=5.3e-12, epochs=2, batch_size=32768 Best score: 0.6970\n",
        "Avazu: lr=1.2e-04, eps=4.1e-07, epochs=2, batch_size=4096 Best score: 0.6592\n",
        "\n",
        "Optimized parameters summary: n_trials=1000\n",
        "CreditCardFraud: lr=1.0e-04, eps=3.1e-12, epochs=1, batch_size=4096 Best score: 0.9599\n",
        "HIGGS: lr=2.9e-01, eps=5.9e-09, epochs=6, batch_size=1024 Best score: 0.6058\n",
        "SUSY: lr=1.7e-04, eps=4.4e-12, epochs=7, batch_size=16384 Best score: 0.8087\n",
        "KDDCup99: lr=2.5e-04, eps=3.9e-10, epochs=6, batch_size=1024 Best score: 0.9965\n",
        "Covertype: lr=9.4e-02, eps=1.0e-11, epochs=1, batch_size=32768 Best score: 0.4176\n",
        "Synthetic: lr=1.6e-04, eps=5.6e-12, epochs=2, batch_size=32768 Best score: 0.6973\n",
        "Avazu: lr=3.2e-04, eps=6.0e-08, epochs=1, batch_size=1024 Best score: 0.6604\n",
        "\n",
        "Optimized parameters summary: n_trials=1000 (optimized method)\n",
        "CreditCardFraud: lr=3.6e-03, eps=5.6e-08, epochs=8, batch_size=8192 Best score: 0.9496\n",
        "HIGGS: lr=5.5e-03, eps=3.2e-09, epochs=8, batch_size=4096 Best score: 0.5443\n",
        "SUSY: lr=1.4e-03, eps=1.9e-09, epochs=10, batch_size=1024 Best score: 0.7116\n",
        "KDDCup99: lr=1.9e-02, eps=9.3e-11, epochs=1, batch_size=8192 Best score: 0.9942\n",
        "Covertype: lr=1.5e-02, eps=5.1e-12, epochs=4, batch_size=65536 Best score: 0.5056\n",
        "Synthetic: lr=1.9e-01, eps=1.4e-11, epochs=3, batch_size=1024 Best score: 0.5558\n",
        "Avazu: lr=5.8e-03, eps=4.8e-07, epochs=7, batch_size=2048 Best score: 0.6153"
      ],
      "metadata": {
        "id": "7Nvig-ThAcly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xEBuVg15Acp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main body opt"
      ],
      "metadata": {
        "id": "TSnIZMGMAkK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_optimized_params(save_path=\"adaptive_bayes_params.json\"):\n",
        "    \"\"\"Загрузить оптимизированные параметры из файла\"\"\"\n",
        "    try:\n",
        "        with open(save_path, 'r') as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Optimized parameters file {save_path} not found. Using defaults.\")\n",
        "        return {}\n",
        "\n",
        "def make_models_with_optimized_params(use_gpu, optimized_params, dataset_name):\n",
        "    \"\"\"\n",
        "    Создать модели с оптимизированными параметрами для AdaptiveBayes\n",
        "\n",
        "    Args:\n",
        "        use_gpu: использовать GPU\n",
        "        optimized_params: словарь оптимизированных параметров\n",
        "        dataset_name: название текущего датасета\n",
        "\n",
        "    Returns:\n",
        "        list: список моделей\n",
        "    \"\"\"\n",
        "    models = []\n",
        "\n",
        "    # AdaptiveBayes с оптимизированными параметрами\n",
        "    try:\n",
        "        params = optimized_params.get(dataset_name, {\n",
        "            'base_lr': 0.01,\n",
        "            'eps': 1e-10,\n",
        "            'epochs': 1,\n",
        "            'batch_size': 65536\n",
        "        })\n",
        "\n",
        "        ab = AdaptiveBayes(\n",
        "            base_lr=params['base_lr'],\n",
        "            eps=params['eps'],\n",
        "            device='gpu' if use_gpu else 'cpu'\n",
        "        )\n",
        "\n",
        "        def fit_with_params(X, y):\n",
        "            return ab.fit(X, y, epochs=params['epochs'], batch_size=params['batch_size'])\n",
        "\n",
        "        models.append((\n",
        "            \"AdaptiveBayes\",\n",
        "            {\n",
        "                \"fit\": fit_with_params,\n",
        "                \"predict\": ab.predict,\n",
        "                \"predict_proba\": ab.predict_proba\n",
        "            }\n",
        "        ))\n",
        "        print(\"AdaptiveBayes model created successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create AdaptiveBayes: {e}\")\n",
        "\n",
        "    # XGBoost\n",
        "    try:\n",
        "        if XGB_OK:\n",
        "            if use_gpu:\n",
        "                params_xgb = {\n",
        "                    \"n_estimators\": 300,\n",
        "                    \"max_depth\": 8,\n",
        "                    \"learning_rate\": 0.1,\n",
        "                    \"subsample\": 0.8,\n",
        "                    \"colsample_bytree\": 0.8,\n",
        "                    \"tree_method\": \"hist\",\n",
        "                    \"device\": \"cuda\",\n",
        "                    \"eval_metric\": \"auc\",\n",
        "                }\n",
        "            else:\n",
        "                params_xgb = {\n",
        "                    \"n_estimators\": 300,\n",
        "                    \"max_depth\": 8,\n",
        "                    \"learning_rate\": 0.1,\n",
        "                    \"subsample\": 0.8,\n",
        "                    \"colsample_bytree\": 0.8,\n",
        "                    \"tree_method\": \"hist\",\n",
        "                    \"eval_metric\": \"auc\",\n",
        "                }\n",
        "\n",
        "            xgbc = xgb.XGBClassifier(**params_xgb)\n",
        "            models.append((\n",
        "                \"XGBoost\",\n",
        "                {\n",
        "                    \"fit\": xgbc.fit,\n",
        "                    \"predict\": xgbc.predict,\n",
        "                    \"predict_proba\": xgbc.predict_proba\n",
        "                }\n",
        "            ))\n",
        "            print(\"XGBoost model created successfully\")\n",
        "        else:\n",
        "            print(\"XGBoost not available (XGB_OK=False)\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create XGBoost: {e}\")\n",
        "\n",
        "    # Random Forest\n",
        "    try:\n",
        "        rf = RandomForestClassifier(n_estimators=300, n_jobs=-1, max_depth=None)\n",
        "        models.append((\n",
        "            \"RandomForest\",\n",
        "            {\n",
        "                \"fit\": rf.fit,\n",
        "                \"predict\": rf.predict,\n",
        "                \"predict_proba\": rf.predict_proba\n",
        "            }\n",
        "        ))\n",
        "        print(\"RandomForest model created successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create RandomForest: {e}\")\n",
        "\n",
        "    # Neural Net (sklearn MLP)\n",
        "    try:\n",
        "        mlp = MLPClassifier(hidden_layer_sizes=(256, 128), batch_size=512, max_iter=20, solver='adam', early_stopping=True, random_state=42)\n",
        "        models.append((\n",
        "            \"MLP\",\n",
        "            {\n",
        "                \"fit\": mlp.fit,\n",
        "                \"predict\": mlp.predict,\n",
        "                \"predict_proba\": mlp.predict_proba\n",
        "            }\n",
        "        ))\n",
        "        print(\"MLP model created successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create MLP: {e}\")\n",
        "\n",
        "    # LightGBM\n",
        "    try:\n",
        "        if LGB_OK:\n",
        "            if use_gpu:\n",
        "                lgbm = lgb.LGBMClassifier(\n",
        "                    n_estimators=300,\n",
        "                    num_leaves=511,\n",
        "                    learning_rate=0.01,\n",
        "                    subsample=0.8,\n",
        "                    colsample_bytree=0.8,\n",
        "                    device_type='gpu',\n",
        "                    max_bin=127,\n",
        "                    min_data_in_leaf=100,\n",
        "                    min_gain_to_split=0.01,\n",
        "                    verbose=-1\n",
        "                )\n",
        "            else:\n",
        "                lgbm = lgb.LGBMClassifier(\n",
        "                    n_estimators=500,\n",
        "                    num_leaves=255,\n",
        "                    learning_rate=0.05,\n",
        "                    subsample=0.8,\n",
        "                    colsample_bytree=0.8,\n",
        "                    device_type='cpu',\n",
        "                    n_jobs=-1,\n",
        "                    verbose=-1\n",
        "                )\n",
        "\n",
        "            models.append((\n",
        "                \"LightGBM\",\n",
        "                {\n",
        "                    \"fit\": lgbm.fit,\n",
        "                    \"predict\": lgbm.predict,\n",
        "                    \"predict_proba\": lgbm.predict_proba\n",
        "                }\n",
        "            ))\n",
        "            print(\"LightGBM model created successfully\")\n",
        "        else:\n",
        "            print(\"LightGBM not available (LGB_OK=False)\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create LightGBM: {e}\")\n",
        "\n",
        "    # CatBoost\n",
        "    try:\n",
        "        if CAT_OK:\n",
        "            cat = CatBoostClassifier(\n",
        "                iterations=500,\n",
        "                depth=8,\n",
        "                learning_rate=0.1,\n",
        "                verbose=False,\n",
        "                task_type=\"GPU\" if use_gpu else \"CPU\"\n",
        "            )\n",
        "            models.append((\n",
        "                \"CatBoost\",\n",
        "                {\n",
        "                    \"fit\": cat.fit,\n",
        "                    \"predict\": cat.predict,\n",
        "                    \"predict_proba\": cat.predict_proba\n",
        "                }\n",
        "            ))\n",
        "            print(\"CatBoost model created successfully\")\n",
        "        else:\n",
        "            print(\"CatBoost not available (CAT_OK=False)\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create CatBoost: {e}\")\n",
        "\n",
        "    # Logistic Regression\n",
        "    try:\n",
        "        lr = LogisticRegression(max_iter=200, solver='saga', n_jobs=-1)\n",
        "        models.append((\n",
        "            \"LogisticRegression\",\n",
        "            {\n",
        "                \"fit\": lr.fit,\n",
        "                \"predict\": lr.predict,\n",
        "                \"predict_proba\": lr.predict_proba\n",
        "            }\n",
        "        ))\n",
        "        print(\"LogisticRegression model created successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create LogisticRegression: {e}\")\n",
        "\n",
        "    print(f\"Total models created: {len(models)}\")\n",
        "    return models\n",
        "\n",
        "# Обновленный run_benchmark\n",
        "def run_benchmark_optimized_new(datasets_config, use_gpu=False, test_size=0.2, optimized_params_path=\"adaptive_bayes_params.json\", output_csv=\"results.csv\"):\n",
        "    \"\"\"\n",
        "    Запуск бенчмарка с использованием оптимизированных параметров\n",
        "    \"\"\"\n",
        "    # Загрузить оптимизированные параметры\n",
        "    optimized_params = load_optimized_params(optimized_params_path)\n",
        "\n",
        "    rows = []\n",
        "    for ds in datasets_config:\n",
        "        name = ds[\"name\"]\n",
        "        loader = ds[\"loader\"]\n",
        "        path = ds[\"path\"]\n",
        "        is_multiclass = ds.get(\"multiclass\", False)\n",
        "        sample_n = ds.get(\"sample_n\")\n",
        "\n",
        "        print(f\"Loading {name} ...\")\n",
        "\n",
        "        # Загрузка данных (код остается тот же)\n",
        "        if name == \"Avazu\":\n",
        "            X, y = load_avazu(path, sample_n=sample_n)\n",
        "        elif name == \"Synthetic\":\n",
        "            X, y = create_synthetic_hepmass()\n",
        "        else:\n",
        "            X, y = loader(path)\n",
        "            if sample_n is not None and len(X) > sample_n:\n",
        "                ridx = np.random.RandomState(42).choice(len(X), size=sample_n, replace=False)\n",
        "                X = X[ridx]\n",
        "                y = y[ridx]\n",
        "\n",
        "        print(f\"{name}: X={X.shape}, y={y.shape}\")\n",
        "\n",
        "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y if not is_multiclass else None)\n",
        "\n",
        "        # Использовать оптимизированные параметры\n",
        "        models = make_models_with_optimized_params(use_gpu=use_gpu, optimized_params=optimized_params, dataset_name=name)\n",
        "\n",
        "        for mname, m in models:\n",
        "            print(f\"Training {mname} on {name} ...\")\n",
        "            stats = train_eval_one_new(mname, m, X_tr, y_tr, X_te, y_te, is_multiclass=is_multiclass, use_gpu=use_gpu)\n",
        "            stats[\"dataset\"] = name\n",
        "            rows.append(stats)\n",
        "            print(stats)\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Saved results to {output_csv}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def train_eval_one_new(model_name, model_ctor, X_train, y_train, X_test, y_test, is_multiclass=False, use_gpu=False):\n",
        "\n",
        "    if hasattr(y_test, 'get'):  # Проверка на CuPy массив\n",
        "        y_test_np = y_test.get()\n",
        "    else:\n",
        "        y_test_np = np.asarray(y_test)\n",
        "\n",
        "    # Нормализация для AdaptiveBayes\n",
        "    if model_name == \"AdaptiveBayes\":\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        fit_stats = _measure_run(model_ctor['fit'], X_train_scaled, y_train)\n",
        "        pred_stats = _measure_run(model_ctor['predict'], X_test_scaled)\n",
        "        y_pred = pred_stats[\"ret\"]\n",
        "        if not is_multiclass and 'predict_proba' in model_ctor:\n",
        "            proba_stats = _measure_run(model_ctor['predict_proba'], X_test_scaled)\n",
        "            y_prob = proba_stats[\"ret\"]\n",
        "            # Преобразовать в NumPy если нужно\n",
        "            if hasattr(y_prob, 'get'):\n",
        "                y_prob = y_prob.get()\n",
        "            auc = roc_auc_score(y_test_np, y_prob)  # Использовать y_test_np\n",
        "            proba_time = proba_stats[\"elapsed_s\"]\n",
        "        else:\n",
        "            proba_time = None\n",
        "            auc = None\n",
        "    else:\n",
        "        fit_stats = _measure_run(model_ctor['fit'], X_train, y_train)\n",
        "        pred_stats = _measure_run(model_ctor['predict'], X_test)\n",
        "        y_pred = pred_stats[\"ret\"]\n",
        "        if not is_multiclass and 'predict_proba' in model_ctor:\n",
        "            proba_stats = _measure_run(model_ctor['predict_proba'], X_test)\n",
        "            y_prob = proba_stats[\"ret\"][:, 1] if y_prob_shape(proba_stats[\"ret\"]) else proba_stats[\"ret\"]\n",
        "            auc = roc_auc_score(y_test_np, y_prob)  # Использовать y_test_np\n",
        "            proba_time = proba_stats[\"elapsed_s\"]\n",
        "        else:\n",
        "            proba_time = None\n",
        "            auc = None\n",
        "\n",
        "    # Преобразовать y_pred в NumPy если нужно\n",
        "    if hasattr(y_pred, 'get'):\n",
        "        y_pred = y_pred.get()\n",
        "\n",
        "    acc = accuracy_score(y_test_np, y_pred)  # Использовать y_test_np\n",
        "\n",
        "    return {\n",
        "        \"model\": model_name,\n",
        "        \"fit_s\": fit_stats[\"elapsed_s\"],\n",
        "        \"pred_s\": pred_stats[\"elapsed_s\"],\n",
        "        \"proba_s\": proba_time,\n",
        "        \"cpu_peak_mb_fit\": fit_stats[\"cpu_peak_mb\"],\n",
        "        \"gpu_mem_mb_fit\": fit_stats[\"gpu_mem_delta_mb\"],\n",
        "        \"acc\": acc,\n",
        "        \"auc\": auc\n",
        "    }\n"
      ],
      "metadata": {
        "id": "DGQobPMYAnQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = [\n",
        "    {\"name\": \"CreditCardFraud\", \"loader\": load_creditcard_fraud, \"path\": \"data/creditcard.csv\", \"multiclass\": False},\n",
        "    {\"name\": \"HIGGS\", \"loader\": load_higgs, \"path\": \"data/HIGGS.csv.gz\", \"multiclass\": False, \"sample_n\": 2_000_000},\n",
        "    {\"name\": \"SUSY\", \"loader\": load_susy, \"path\": \"data/SUSY.csv.gz\", \"multiclass\": False, \"sample_n\": 2_000_000},\n",
        "    {\"name\": \"KDDCup99\", \"loader\": load_kddcup99, \"path\": \"data/kddcup.data_10_percent.csv\", \"multiclass\": False},\n",
        "    {\"name\": \"Covertype\", \"loader\": load_covertype, \"path\": \"data/covtype.csv\", \"multiclass\": True},\n",
        "    {\"name\": \"Synthetic\", \"loader\": create_synthetic_hepmass, \"path\": \"\", \"multiclass\": False, \"sample_n\": 2_000_000},\n",
        "    {\"name\": \"Avazu\", \"loader\": load_avazu, \"path\": \"data/avazu-ctr-train.zip\", \"multiclass\": False, \"sample_n\": 2_000_000},\n",
        "]\n",
        "\n",
        "use_gpu = GPU_OK\n",
        "print(\"\\nStarting benchmark with optimized parameters...\")\n",
        "results_df = run_benchmark_optimized_new(\n",
        "    datasets,\n",
        "    use_gpu=use_gpu,\n",
        "    optimized_params_path=\"adaptive_bayes_params.json\",\n",
        "    output_csv=\"benchmark_results_optimized.csv\"\n",
        ")\n",
        "\n",
        "print(\"Benchmark completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvIBKXgNZaHX",
        "outputId": "ce7855ae-d967-4160-8c51-b1306a93e46f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting benchmark with optimized parameters...\n",
            "Loading CreditCardFraud ...\n",
            "CreditCardFraud: X=(284807, 30), y=(284807,)\n",
            "AdaptiveBayes model created successfully\n",
            "XGBoost model created successfully\n",
            "RandomForest model created successfully\n",
            "MLP model created successfully\n",
            "LightGBM model created successfully\n",
            "CatBoost not available (CAT_OK=False)\n",
            "LogisticRegression model created successfully\n",
            "Total models created: 6\n",
            "Training AdaptiveBayes on CreditCardFraud ...\n",
            "{'model': 'AdaptiveBayes', 'fit_s': 0.2469200900013675, 'pred_s': 0.21935137100081192, 'proba_s': 0.2202568060001795, 'cpu_peak_mb_fit': 0.05859375, 'gpu_mem_mb_fit': 0.0, 'acc': 0.40881640391840174, 'auc': np.float64(0.9228133290457432), 'dataset': 'CreditCardFraud'}\n",
            "Training XGBoost on CreditCardFraud ...\n",
            "{'model': 'XGBoost', 'fit_s': 1.181520260999605, 'pred_s': 0.22560092200001236, 'proba_s': 0.225835510998877, 'cpu_peak_mb_fit': 0.79296875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9995435553526912, 'auc': np.float64(0.9702255937546657), 'dataset': 'CreditCardFraud'}\n",
            "Training RandomForest on CreditCardFraud ...\n",
            "{'model': 'RandomForest', 'fit_s': 112.94871176000015, 'pred_s': 0.4105171869996411, 'proba_s': 0.40696077599932323, 'cpu_peak_mb_fit': 0.0, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9995786664794073, 'auc': np.float64(0.9668779178103432), 'dataset': 'CreditCardFraud'}\n",
            "Training MLP on CreditCardFraud ...\n",
            "{'model': 'MLP', 'fit_s': 32.0319418930012, 'pred_s': 1.0550664229995164, 'proba_s': 1.1184161919991311, 'cpu_peak_mb_fit': 0.06640625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9982795547909132, 'auc': np.float64(0.5045023285059663), 'dataset': 'CreditCardFraud'}\n",
            "Training LightGBM on CreditCardFraud ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LightGBM', 'fit_s': 3.7661205639997206, 'pred_s': 0.5097568729997874, 'proba_s': 0.5361108019988023, 'cpu_peak_mb_fit': 76.40625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9995962220427653, 'auc': np.float64(0.9647341526650053), 'dataset': 'CreditCardFraud'}\n",
            "Training LogisticRegression on CreditCardFraud ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LogisticRegression', 'fit_s': 37.526423876999615, 'pred_s': 0.24674821599910501, 'proba_s': 0.26229485600015323, 'cpu_peak_mb_fit': 0.078125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9982795547909132, 'auc': np.float64(0.6287714403431603), 'dataset': 'CreditCardFraud'}\n",
            "Loading HIGGS ...\n",
            "HIGGS: X=(2000000, 28), y=(2000000,)\n",
            "AdaptiveBayes model created successfully\n",
            "XGBoost model created successfully\n",
            "RandomForest model created successfully\n",
            "MLP model created successfully\n",
            "LightGBM model created successfully\n",
            "CatBoost not available (CAT_OK=False)\n",
            "LogisticRegression model created successfully\n",
            "Total models created: 6\n",
            "Training AdaptiveBayes on HIGGS ...\n",
            "{'model': 'AdaptiveBayes', 'fit_s': 0.5976915699993697, 'pred_s': 0.385136605000298, 'proba_s': 0.384697823001261, 'cpu_peak_mb_fit': 0.0, 'gpu_mem_mb_fit': 0.0, 'acc': 0.47949, 'auc': np.float64(0.4980634711957723), 'dataset': 'HIGGS'}\n",
            "Training XGBoost on HIGGS ...\n",
            "{'model': 'XGBoost', 'fit_s': 5.410127348999595, 'pred_s': 0.3837816709983599, 'proba_s': 0.3794244159998925, 'cpu_peak_mb_fit': 2.25, 'gpu_mem_mb_fit': 0.0, 'acc': 0.75044, 'auc': np.float64(0.8333018787578815), 'dataset': 'HIGGS'}\n",
            "Training RandomForest on HIGGS ...\n",
            "{'model': 'RandomForest', 'fit_s': 798.4104671330006, 'pred_s': 9.837557802000447, 'proba_s': 9.581183825999688, 'cpu_peak_mb_fit': 7244.6953125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.743275, 'auc': np.float64(0.8251283253909765), 'dataset': 'HIGGS'}\n",
            "Training MLP on HIGGS ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'MLP', 'fit_s': 494.6359210270002, 'pred_s': 3.637208427000587, 'proba_s': 3.9900994330000685, 'cpu_peak_mb_fit': 776.40625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.7568575, 'auc': np.float64(0.8392266119082334), 'dataset': 'HIGGS'}\n",
            "Training LightGBM on HIGGS ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LightGBM', 'fit_s': 32.70174450800005, 'pred_s': 6.0014687109996885, 'proba_s': 5.937091752000924, 'cpu_peak_mb_fit': 77.9140625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.738545, 'auc': np.float64(0.8198907346103645), 'dataset': 'HIGGS'}\n",
            "Training LogisticRegression on HIGGS ...\n",
            "{'model': 'LogisticRegression', 'fit_s': 19.973109348000435, 'pred_s': 0.40474446799998987, 'proba_s': 0.4104644199996983, 'cpu_peak_mb_fit': 0.0703125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.64199, 'auc': np.float64(0.6850410677346743), 'dataset': 'HIGGS'}\n",
            "Loading SUSY ...\n",
            "SUSY: X=(2000000, 18), y=(2000000,)\n",
            "AdaptiveBayes model created successfully\n",
            "XGBoost model created successfully\n",
            "RandomForest model created successfully\n",
            "MLP model created successfully\n",
            "LightGBM model created successfully\n",
            "CatBoost not available (CAT_OK=False)\n",
            "LogisticRegression model created successfully\n",
            "Total models created: 6\n",
            "Training AdaptiveBayes on SUSY ...\n",
            "{'model': 'AdaptiveBayes', 'fit_s': 1.7281565590001264, 'pred_s': 0.36536286099908466, 'proba_s': 0.36453352900025493, 'cpu_peak_mb_fit': 0.0, 'gpu_mem_mb_fit': 0.0, 'acc': 0.5516225, 'auc': np.float64(0.5472659067528088), 'dataset': 'SUSY'}\n",
            "Training XGBoost on SUSY ...\n",
            "{'model': 'XGBoost', 'fit_s': 3.967151613000169, 'pred_s': 0.3741035450002528, 'proba_s': 0.37622822399862343, 'cpu_peak_mb_fit': 0.1484375, 'gpu_mem_mb_fit': 0.0, 'acc': 0.803625, 'auc': np.float64(0.876170485370527), 'dataset': 'SUSY'}\n",
            "Training RandomForest on SUSY ...\n",
            "{'model': 'RandomForest', 'fit_s': 846.0003864700011, 'pred_s': 9.248954431999664, 'proba_s': 8.94064137100031, 'cpu_peak_mb_fit': 2733.8984375, 'gpu_mem_mb_fit': 0.0, 'acc': 0.8012925, 'auc': np.float64(0.8719865652678915), 'dataset': 'SUSY'}\n",
            "Training MLP on SUSY ...\n",
            "{'model': 'MLP', 'fit_s': 383.04493522899975, 'pred_s': 2.7532906369997363, 'proba_s': 2.83398877000036, 'cpu_peak_mb_fit': 666.6796875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.8032725, 'auc': np.float64(0.8767305105543572), 'dataset': 'SUSY'}\n",
            "Training LightGBM on SUSY ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LightGBM', 'fit_s': 28.4404950320004, 'pred_s': 5.763161061000574, 'proba_s': 5.974724818999675, 'cpu_peak_mb_fit': 76.59765625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.80267, 'auc': np.float64(0.8752886438380781), 'dataset': 'SUSY'}\n",
            "Training LogisticRegression on SUSY ...\n",
            "{'model': 'LogisticRegression', 'fit_s': 43.01121556399994, 'pred_s': 0.4061513649994595, 'proba_s': 0.4335690780008008, 'cpu_peak_mb_fit': 0.0625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.7887375, 'auc': np.float64(0.8580859056565515), 'dataset': 'SUSY'}\n",
            "Loading KDDCup99 ...\n",
            "KDDCup99: X=(494021, 37), y=(494021,)\n",
            "AdaptiveBayes model created successfully\n",
            "XGBoost model created successfully\n",
            "RandomForest model created successfully\n",
            "MLP model created successfully\n",
            "LightGBM model created successfully\n",
            "CatBoost not available (CAT_OK=False)\n",
            "LogisticRegression model created successfully\n",
            "Total models created: 6\n",
            "Training AdaptiveBayes on KDDCup99 ...\n",
            "{'model': 'AdaptiveBayes', 'fit_s': 0.30553325500113715, 'pred_s': 0.3271617000009428, 'proba_s': 0.3293577809999988, 'cpu_peak_mb_fit': 0.0, 'gpu_mem_mb_fit': 0.0, 'acc': 0.8424067607914579, 'auc': np.float64(0.9069642077330262), 'dataset': 'KDDCup99'}\n",
            "Training XGBoost on KDDCup99 ...\n",
            "{'model': 'XGBoost', 'fit_s': 1.482274823998523, 'pred_s': 0.33145727300143335, 'proba_s': 0.33960180599933665, 'cpu_peak_mb_fit': 0.0078125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9998178229846668, 'auc': np.float64(0.9999987349513493), 'dataset': 'KDDCup99'}\n",
            "Training RandomForest on KDDCup99 ...\n",
            "{'model': 'RandomForest', 'fit_s': 15.42372675200022, 'pred_s': 0.48367153700019117, 'proba_s': 0.4807985349998489, 'cpu_peak_mb_fit': 31.98828125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9997975810940742, 'auc': np.float64(0.9999996395291479), 'dataset': 'KDDCup99'}\n",
            "Training MLP on KDDCup99 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'MLP', 'fit_s': 96.47967880000033, 'pred_s': 1.754981609999959, 'proba_s': 1.7602468989989575, 'cpu_peak_mb_fit': 0.0625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9969839583017054, 'auc': np.float64(0.9985354687876211), 'dataset': 'KDDCup99'}\n",
            "Training LightGBM on KDDCup99 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LightGBM', 'fit_s': 5.330835988999752, 'pred_s': 0.6704027059986402, 'proba_s': 0.6594466009992175, 'cpu_peak_mb_fit': 76.40234375, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9995749202975558, 'auc': np.float64(0.9999978501298146), 'dataset': 'KDDCup99'}\n",
            "Training LogisticRegression on KDDCup99 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LogisticRegression', 'fit_s': 75.62943199000074, 'pred_s': 0.3758281829996122, 'proba_s': 0.37654738099990936, 'cpu_peak_mb_fit': 0.06640625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.8891149233338393, 'auc': np.float64(0.8605965194473565), 'dataset': 'KDDCup99'}\n",
            "Loading Covertype ...\n",
            "Covertype: X=(581011, 54), y=(581011,)\n",
            "AdaptiveBayes model created successfully\n",
            "XGBoost model created successfully\n",
            "RandomForest model created successfully\n",
            "MLP model created successfully\n",
            "LightGBM model created successfully\n",
            "CatBoost not available (CAT_OK=False)\n",
            "LogisticRegression model created successfully\n",
            "Total models created: 6\n",
            "Training AdaptiveBayes on Covertype ...\n",
            "{'model': 'AdaptiveBayes', 'fit_s': 0.34974628499912797, 'pred_s': 0.3744307390006725, 'proba_s': None, 'cpu_peak_mb_fit': 0.0, 'gpu_mem_mb_fit': 0.0, 'acc': 0.43421426297083554, 'auc': None, 'dataset': 'Covertype'}\n",
            "Training XGBoost on Covertype ...\n",
            "{'model': 'XGBoost', 'fit_s': 11.489693601000909, 'pred_s': 0.44690461400023196, 'proba_s': None, 'cpu_peak_mb_fit': 0.0078125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9197352908272592, 'auc': None, 'dataset': 'Covertype'}\n",
            "Training RandomForest on Covertype ...\n",
            "{'model': 'RandomForest', 'fit_s': 74.68684863300041, 'pred_s': 2.0310562039994693, 'proba_s': None, 'cpu_peak_mb_fit': 446.9765625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9564038794179152, 'auc': None, 'dataset': 'Covertype'}\n",
            "Training MLP on Covertype ...\n",
            "{'model': 'MLP', 'fit_s': 104.97767958099939, 'pred_s': 2.048356410001361, 'proba_s': None, 'cpu_peak_mb_fit': 0.11328125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.7401013743190795, 'auc': None, 'dataset': 'Covertype'}\n",
            "Training LightGBM on Covertype ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LightGBM', 'fit_s': 81.84064850600043, 'pred_s': 10.531017110000903, 'proba_s': None, 'cpu_peak_mb_fit': 76.40234375, 'gpu_mem_mb_fit': 0.0, 'acc': 0.9346660585354939, 'auc': None, 'dataset': 'Covertype'}\n",
            "Training LogisticRegression on Covertype ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LogisticRegression', 'fit_s': 227.3571067140001, 'pred_s': 0.39441068899941456, 'proba_s': None, 'cpu_peak_mb_fit': 0.0546875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.6920905656480469, 'auc': None, 'dataset': 'Covertype'}\n",
            "Loading Synthetic ...\n",
            "Creating synthetic HEPMASS-like dataset...\n",
            "Synthetic dataset: X=(500000, 28), y=(500000,), class balance=0.615\n",
            "Synthetic: X=(500000, 28), y=(500000,)\n",
            "AdaptiveBayes model created successfully\n",
            "XGBoost model created successfully\n",
            "RandomForest model created successfully\n",
            "MLP model created successfully\n",
            "LightGBM model created successfully\n",
            "CatBoost not available (CAT_OK=False)\n",
            "LogisticRegression model created successfully\n",
            "Total models created: 6\n",
            "Training AdaptiveBayes on Synthetic ...\n",
            "{'model': 'AdaptiveBayes', 'fit_s': 0.41583776099832903, 'pred_s': 0.329632037999545, 'proba_s': 0.3275539080004819, 'cpu_peak_mb_fit': 0.0, 'gpu_mem_mb_fit': 0.0, 'acc': 0.46689, 'auc': np.float64(0.46462116407482046), 'dataset': 'Synthetic'}\n",
            "Training XGBoost on Synthetic ...\n",
            "{'model': 'XGBoost', 'fit_s': 2.5378600519998145, 'pred_s': 0.3497018759990169, 'proba_s': 0.3389458210003795, 'cpu_peak_mb_fit': 0.1953125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.68526, 'auc': np.float64(0.7385931095177556), 'dataset': 'Synthetic'}\n",
            "Training RandomForest on Synthetic ...\n",
            "{'model': 'RandomForest', 'fit_s': 259.00877690200105, 'pred_s': 1.9742707420009538, 'proba_s': 1.978895845999432, 'cpu_peak_mb_fit': 541.8046875, 'gpu_mem_mb_fit': 0.0, 'acc': 0.68013, 'auc': np.float64(0.7275880164044531), 'dataset': 'Synthetic'}\n",
            "Training MLP on Synthetic ...\n",
            "{'model': 'MLP', 'fit_s': 74.85405983999954, 'pred_s': 2.24954988600075, 'proba_s': 1.8660093899998174, 'cpu_peak_mb_fit': 0.23828125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.68963, 'auc': np.float64(0.7444281658870684), 'dataset': 'Synthetic'}\n",
            "Training LightGBM on Synthetic ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LightGBM', 'fit_s': 16.566909887000293, 'pred_s': 1.4031730289989355, 'proba_s': 1.4045511129988881, 'cpu_peak_mb_fit': 76.41015625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.67988, 'auc': np.float64(0.731728763262864), 'dataset': 'Synthetic'}\n",
            "Training LogisticRegression on Synthetic ...\n",
            "{'model': 'LogisticRegression', 'fit_s': 4.282861450999917, 'pred_s': 0.3596518609992927, 'proba_s': 0.36505795099947136, 'cpu_peak_mb_fit': 0.0703125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.67195, 'auc': np.float64(0.7195245912402302), 'dataset': 'Synthetic'}\n",
            "Loading Avazu ...\n",
            "Avazu: X=(1000000, 23), y=(1000000,)\n",
            "AdaptiveBayes model created successfully\n",
            "XGBoost model created successfully\n",
            "RandomForest model created successfully\n",
            "MLP model created successfully\n",
            "LightGBM model created successfully\n",
            "CatBoost not available (CAT_OK=False)\n",
            "LogisticRegression model created successfully\n",
            "Total models created: 6\n",
            "Training AdaptiveBayes on Avazu ...\n",
            "{'model': 'AdaptiveBayes', 'fit_s': 0.5979644610015384, 'pred_s': 0.36910307400103193, 'proba_s': 0.37124670400044124, 'cpu_peak_mb_fit': 0.0, 'gpu_mem_mb_fit': 0.0, 'acc': 0.531285, 'auc': np.float64(0.46212340201511304), 'dataset': 'Avazu'}\n",
            "Training XGBoost on Avazu ...\n",
            "{'model': 'XGBoost', 'fit_s': 2.346432165999431, 'pred_s': 0.39317114199911884, 'proba_s': 0.3825453980007296, 'cpu_peak_mb_fit': 0.0078125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.8443, 'auc': np.float64(0.7656538853155123), 'dataset': 'Avazu'}\n",
            "Training RandomForest on Avazu ...\n",
            "{'model': 'RandomForest', 'fit_s': 141.7829807729995, 'pred_s': 4.376341413000773, 'proba_s': 4.21639490799862, 'cpu_peak_mb_fit': 622.33203125, 'gpu_mem_mb_fit': 0.0, 'acc': 0.83491, 'auc': np.float64(0.7487904648468557), 'dataset': 'Avazu'}\n",
            "Training MLP on Avazu ...\n",
            "{'model': 'MLP', 'fit_s': 110.56860421800047, 'pred_s': 2.826000766999641, 'proba_s': 3.3781428929996764, 'cpu_peak_mb_fit': 9.8359375, 'gpu_mem_mb_fit': 0.0, 'acc': 0.83978, 'auc': np.float64(0.5), 'dataset': 'Avazu'}\n",
            "Training LightGBM on Avazu ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'LightGBM', 'fit_s': 21.676110870999764, 'pred_s': 3.6098453319991677, 'proba_s': 3.40433548999863, 'cpu_peak_mb_fit': 76.40234375, 'gpu_mem_mb_fit': 0.0, 'acc': 0.84404, 'auc': np.float64(0.7657017974967372), 'dataset': 'Avazu'}\n",
            "Training LogisticRegression on Avazu ...\n",
            "{'model': 'LogisticRegression', 'fit_s': 8.048615167999742, 'pred_s': 0.3574902239997755, 'proba_s': 0.3495863419993839, 'cpu_peak_mb_fit': 0.06640625, 'gpu_mem_mb_fit': 0.0, 'acc': 0.83978, 'auc': np.float64(0.5020758517711775), 'dataset': 'Avazu'}\n",
            "Saved results to benchmark_results_optimized.csv\n",
            "Benchmark completed!\n"
          ]
        }
      ]
    }
  ]
}